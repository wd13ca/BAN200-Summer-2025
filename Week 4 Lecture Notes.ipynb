{"cells":[{"cell_type":"markdown","id":"39843489","metadata":{"id":"39843489"},"source":["# Week 4 Lecture Notes\n","\n","This week, we built our first **supervised machine learning classifier** from scratch â€” a Naive Bayes model for **spam detection**. Along the way, we compared rule-based and learning-based approaches, reviewed key ML concepts, and connected everything back to ideas we've already explored in sentiment analysis. By the end of this unit, youâ€™ll understand not just how Naive Bayes works, but why it works â€” and how surprisingly similar it is to the lexicon-based classifier we built earlier.\n","\n","## Rule-Based vs. Machine Learningâ€“Based Systems\n","\n","Before we dive into building our first *machine learning model*, letâ€™s review the difference between two major approaches to analyzing text:\n","\n","### Rule-Based Systems\n","\n","Rule-based systems rely on **handcrafted logic** to analyze text. You define the rules, and the system applies them.\n","\n","**Examples:**\n","\n","- If a message contains the word â€œrefundâ€, label it as a complaint.\n","- If a review contains more positive than negative words (from a lexicon), predict it as positive.\n","\n","Weâ€™ve already seen some rule-based systems in this course:\n","\n","- **Information retrieval** using cosine similarity and TF-IDF is rule-based â€” it follows a fixed mathematical formula to rank documents.\n","- **Lexicon-based sentiment analysis** is rule-based â€” it assigns scores to documents based on a predefined dictionary of word sentiments. *Note: you could use machine learning to create the lexicon (e.g., by training on labeled data), but our approach was purely rule-based.*\n","\n","**Pros:**\n","\n","- Simple, interpretable, and easy to build\n","- No training data needed\n","- Good for clear-cut or highly structured tasks\n","\n","**Cons:**\n","\n","- Rigid and brittle â€” can break with new wording or phrasing\n","- Canâ€™t handle nuance or subtlety (e.g., sarcasm, negation)\n","- Hard to scale and maintain as complexity grows\n","\n","### Machine Learningâ€“Based Systems\n","\n","Machine learning systems **learn patterns from labeled examples** instead of being explicitly told what to do.\n","\n","You give the model **input texts** and the correct **output labels**, and it finds patterns that connect them.\n","\n","**Examples:**\n","\n","- Train a model on 10,000 emails labeled â€œspamâ€ or â€œnot spamâ€\n","- Use word frequency patterns to automatically learn what spam looks like\n","- Predict new labels for unseen messages based on learned patterns\n","\n","Weâ€™ve already seen a machine learning system:\n","\n","- **K-Means clustering** is an unsupervised ML model â€” it learns to group similar documents without predefined categories.\n","\n","**Pros:**\n","\n","- Flexible and adaptive â€” learns from data, not rules\n","- Can capture subtle statistical signals\n","- Scales well to large datasets and complex problems\n","\n","**Cons:**\n","\n","- Needs labeled data to train (in supervised learning)\n","- Can be harder to interpret\n","- May make mistakes in unexpected ways\n","\n","## Supervised vs. Unsupervised Learning\n","\n","Now that weâ€™ve reviewed machine learning systems, letâ€™s look at two major types of learning: **supervised** and **unsupervised**.\n","\n","### Unsupervised Learning\n","\n","In **unsupervised learning**, the model is given input data **without any labels**. Its goal is to **find patterns or structure** in the data on its own.\n","\n","**Example: K-Means Clustering**\n","\n","- We gave K-Means a set of review vectors â€” but **no labels** or categories.\n","- The model grouped similar reviews into clusters based on shared vocabulary.\n","- We didnâ€™t tell the model what the clusters should be â€” it discovered patterns automatically.\n","\n","Unsupervised learning is useful when:\n","- You donâ€™t have labeled data\n","- You want to explore or summarize large datasets\n","- You want to discover hidden structure or themes\n","\n","### Supervised Learning\n","\n","In **supervised learning**, the model is trained on input data **with known labels**. It learns the relationship between the input and the correct output, so it can make predictions on new data.\n","\n","**What Weâ€™re Doing Today: Naive Bayes**\n","\n","- We'll build a **spam detection model** using a dataset of messages labeled as **spam** or **not spam**.\n","- The model will learn what kinds of words are commonly found in spam messages vs. regular ones.\n","- Once trained, the model can classify **new, unseen messages** as spam or not.\n","\n","> **Key Idea**:  \n","> Supervised learning requires labeled data and predicts known outcomes.  \n","> Unsupervised learning looks for patterns without any guidance.\n","\n","\n","## Introducing Naive Bayes: A Machine Learning Approach to Text Classification\n","\n","So far, weâ€™ve built a **lexicon-based sentiment classifier**:  \n","\n","- Every word in a review had a predefined **sentiment score** (positive, negative, or neutral).  \n","- To score a review, we **added up the scores of the words it contained**.  \n","- Based on the total score, we classified the review as **positive, neutral, or negative**.\n","\n","Today, weâ€™ll build a new kind of text classifier â€” using **machine learning**.  \n","\n","Our task is to classify messages as either **spam** or **not spam** (also called *ham*).  \n","\n","Weâ€™ll use a model called **Naive Bayes**, which is commonly used for spam detection.\n","\n","### Similarities to the Lexicon Model\n","\n","In both models:\n","\n","- Each message is represented as a **bag of words**\n","- Each word contributes to the **final message score**\n","- The final score determines the predicted **class label**:\n","  - Positive vs. Negative (in sentiment analysis)\n","  - Spam vs. Not Spam (in todayâ€™s example)\n","\n","### Key Difference: Where the Word Scores Come From\n","\n","- In the **lexicon model**, the word scores were **manually defined** in a sentiment dictionary.  \n","- In the **Naive Bayes model**, the word scores will be **learned from data**.  \n","  - Weâ€™ll train the model on **labeled messages** â€” ones that are already marked as spam or not.\n","  - The model will learn which words are more likely to appear in spam vs. ham.\n","\n","This is what makes Naive Bayes a **supervised machine learning model**.\n","\n","> **Key Idea**:  \n","> Just like our lexicon-based model, Naive Bayes uses word-level scores to make a prediction â€”  \n","> but it **learns** those scores automatically by analyzing a labeled dataset.\n","\n","## Deriving Naive Bayes for Spam Detection\n","\n","To understand how our model works, weâ€™ll start with **Bayesâ€™ Rule** â€” a fundamental idea in probability theory.\n","\n","### Bayesâ€™ Rule\n","\n","Bayesâ€™ Rule helps us reverse conditional probabilities:\n","\n","**P(A | B) = P(B | A) * P(A) / P(B)**\n","\n","In our case:\n","\n","- **A** is the class (e.g., \"spam\" or \"not spam\")\n","- **B** is the message text\n","\n","We want to calculate:\n","\n","**P(spam | message)** â€” the probability that a message is spam, given its contents.\n","\n","Using Bayesâ€™ Rule:\n","\n","**P(spam | message) = P(message | spam) * P(spam) / P(message)**\n","\n","And likewise:\n","\n","**P(ham | message) = P(message | ham) * P(ham) / P(message)**\n","\n","To classify the message, we compare these two probabilities and choose the class with the higher value.\n","\n","### The Naive Assumption\n","\n","The tricky part is computing **P(message | spam)** â€” the probability of the entire message, given that itâ€™s spam.\n","\n","Since messages are made up of many words, this would normally be very hard to compute.  \n","So we make a simplifying assumption:\n","\n","> **Naive Bayes assumes that all words in a message are independent, given the class.**\n","\n","That means:\n","\n","**P(message | spam)**  \n","â‰ˆ **P(w1 | spam) * P(w2 | spam) * ... * P(wn | spam)**\n","\n","We do the same for ham:\n","\n","**P(message | ham)**  \n","â‰ˆ **P(w1 | ham) * P(w2 | ham) * ... * P(wn | ham)**\n","\n","### The Final Classification Rule\n","\n","Now we can skip the denominator (P(message)) because itâ€™s the same for both classes.\n","\n","To classify a message, we compute:\n","\n","- **spam score = P(spam) * P(w1 | spam) * P(w2 | spam) * ...**\n","- **ham score = P(ham) * P(w1 | ham) * P(w2 | ham) * ...**\n","\n","Then we pick the class with the higher score.\n","\n","### Take the Log\n","\n","Multiplying lots of small probabilities (like 0.01 Ã— 0.005 Ã— 0.0008...) can lead to **very tiny numbers** that computers have trouble storing.  \n","\n","Instead, we take the **logarithm** of the scores, which turns multiplication into addition:\n","\n","**log(spam score) = log(P(spam)) + log(P(w1 | spam)) + log(P(w2 | spam)) + ...**\n","\n","And the same for ham:\n","\n","**log(ham score) = log(P(ham)) + log(P(w1 | ham)) + log(P(w2 | ham)) + ...**\n","\n","This gives us a much more stable calculation â€” and itâ€™s also easier to interpret.\n","\n","### Final Classification Rule\n","\n","We subtract the two log-scores and check the sign of the result:\n","\n","**score = log(spam score) - log(ham score)**  \n","= **log(P(spam) / P(ham))**  \n","  + **sum over words: log(P(word | spam) / P(word | ham))**\n","\n","This gives us a **single number** that summarizes how \"spammy\" the message is.\n","\n","### How to Make a Prediction\n","\n","- If the score is **greater than 0**, the message is **more likely spam**\n","- If the score is **less than 0**, the message is **more likely ham (not spam)**\n","- If the score is **exactly 0**, the model is completely uncertain (rare)\n","\n","\n","### Why This Looks Like a Lexicon Model\n","\n","- In lexicon-based sentiment analysis, we added up the **sentiment scores** of each word.\n","- In Naive Bayes (after taking logs), we add up the **spaminess scores** of each word:\n","  - For each word: **log(P(word | spam) / P(word | ham))**\n","\n","This becomes our **spam lexicon**, but instead of being handcrafted, it's **learned from data**.\n","\n","### What About the Constant?\n","\n","In our log-based Naive Bayes classifier, the full formula looks like this:\n","\n","**score = log(P(spam) / P(ham))  \n","â€ƒâ€ƒ+ sum over words: log(P(word | spam) / P(word | ham))**\n","\n","The first part â€” **log(P(spam) / P(ham))** â€” is a **constant**. It doesnâ€™t depend on the message or its words. It just reflects how common spam is in the training data compared to ham.\n","\n","#### What Does It Mean?\n","\n","- If spam is more common than ham in the training set, the constant will be **positive**\n","- If ham is more common, the constant will be **negative**\n","\n","This constant shifts the final score **up or down** and acts as a **bias term**:\n","- A high constant makes the model more likely to predict spam (unless the words strongly suggest otherwise)\n","- A low constant makes the model more conservative (only labeling as spam if the words are very spammy)\n","\n","#### Why Donâ€™t We Have This in Lexicon-Based Sentiment?\n","\n","In our sentiment model, we only use the sum of word scores â€” thereâ€™s **no class prior** built into the model.\n","\n","That means:\n","- We assume that positive and negative reviews are equally likely by default\n","- There's **no bias toward one label or the other** unless the words push it in that direction\n","\n","If we wanted to, we could **add a constant** to our sentiment model to reflect prior expectations â€” for example, if we know most reviews tend to be positive â€” but in practice, we usually leave it out for simplicity.\n","\n","## SMS Spam Collection\n","\n","Now that we understand how the Naive Bayes model works, letâ€™s put it into practice.\n","\n","Weâ€™ll be using a real-world dataset called the **SMS Spam Collection**. It contains **5,574 text messages**, each labeled as either:\n","\n","- **\"spam\"** â€” unwanted commercial messages, scams, or promotions\n","- **\"ham\"** â€” regular, non-spam messages (e.g., from friends, family, or service providers)\n","\n","This dataset is widely used to teach and evaluate text classification models, and it's perfect for our goal today.\n","\n","### Our Objective\n","\n","We want to **train a Naive Bayes classifier** that can automatically determine whether a new message is spam or not.\n","\n","To do that, weâ€™ll:\n","\n","1. **Load the dataset**\n","2. **Tokenize the messages**\n","3. **Estimate word probabilities for each class**\n","4. **Compute log-scores for new messages**\n","5. **Make predictions and evaluate accuracy**\n","\n","This is a classic example of **supervised learning**:\n","\n","- We train the model on a set of **labeled examples**\n","- The model learns which words are more likely to appear in spam vs. ham\n","- It uses this to classify **new, unseen messages**\n","\n","Letâ€™s get started by loading and exploring the data.\n","\n","## Download and Load the Dataset\n","\n","The SMS Spam Collection is available as a **tab-delimited text file** where:\n","\n","- The first column is the **label** (`ham` or `spam`)\n","- The second column is the **message text**\n","\n","Weâ€™ll download the file using `wget`, then read it into a DataFrame using `pandas`."]},{"cell_type":"code","execution_count":1,"id":"ee3d8f89","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"ee3d8f89","executionInfo":{"status":"ok","timestamp":1747954528348,"user_tz":240,"elapsed":1779,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"afe332ec-56f2-48c3-9b80-deb359de20e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-22 22:55:26--  https://storage.googleapis.com/wd13/SMSSpamCollection.txt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.16.207, 142.251.163.207, 142.251.167.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.16.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 477907 (467K) [text/plain]\n","Saving to: â€˜SMSSpamCollection.txtâ€™\n","\n","\rSMSSpamCollection.t   0%[                    ]       0  --.-KB/s               \rSMSSpamCollection.t 100%[===================>] 466.71K  --.-KB/s    in 0.01s   \n","\n","2025-05-22 22:55:26 (42.8 MB/s) - â€˜SMSSpamCollection.txtâ€™ saved [477907/477907]\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["  label                                            message\n","0   ham  Go until jurong point, crazy.. Available only ...\n","1   ham                      Ok lar... Joking wif u oni...\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","3   ham  U dun say so early hor... U c already then say...\n","4   ham  Nah I don't think he goes to usf, he lives aro..."],"text/html":["\n","  <div id=\"df-5f91c421-427b-453e-b3d3-2bfe38eb71b6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f91c421-427b-453e-b3d3-2bfe38eb71b6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5f91c421-427b-453e-b3d3-2bfe38eb71b6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5f91c421-427b-453e-b3d3-2bfe38eb71b6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-0e39fc7a-054b-471f-8b5a-1c3898104844\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e39fc7a-054b-471f-8b5a-1c3898104844')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-0e39fc7a-054b-471f-8b5a-1c3898104844 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":1}],"source":["# Download the dataset\n","!wget https://storage.googleapis.com/wd13/SMSSpamCollection.txt\n","\n","# Load it into a pandas DataFrame\n","import pandas as pd\n","\n","df = pd.read_csv(\"SMSSpamCollection.txt\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n","\n","# Display the first few rows\n","df.head()"]},{"cell_type":"markdown","id":"8f0e45e7","metadata":{"id":"8f0e45e7"},"source":["Each row represents one SMS message, labeled as either `\"ham\"` or `\"spam\"`. We'll use this DataFrame as our training data for the Naive Bayes classifier.\n","\n","### A Quick Note on Pandas and DataFrames\n","\n","In this example, we're using a Python library called **pandas**. It's one of the most popular tools for working with data in Python.\n","\n","The main structure in pandas is the **DataFrame** â€” a table of data with **rows and columns**, similar to a spreadsheet or a SQL table.\n","\n","When we load our SMS dataset using `pd.read_csv()`, we get a DataFrame where:\n","\n","- Each **row** is one text message\n","- Each **column** holds a different kind of information (like the label or the message text)\n","\n","For example:\n","\n","| label | message                                  |\n","|-------|------------------------------------------|\n","| ham   | Go until jurong point, crazy..           |\n","| ham   | Ok lar... Joking wif u oni...            |\n","| spam  | Free entry in 2 a wkly comp to win FA... |\n","\n","We can inspect the first few rows of any DataFrame using `.head()`.\n","\n","## Split Into Training and Test Sets\n","\n","Before we train our Naive Bayes model, we need to split our dataset into two parts:\n","\n","- **Training set**: The portion of the data we use to **teach** the model. It learns which words are common in spam vs. ham.\n","- **Test set**: A separate portion that we use to **evaluate** the model â€” to see how well it performs on new, unseen messages.\n","\n","*Why do we split the data?* If we evaluate the model on the same data it was trained on, weâ€™re not really testing its ability to generalize.  To truly see how well it works, we need to test it on data it hasnâ€™t seen â€” just like how it would be used in the real world.\n","\n","To do this, weâ€™ll use a function called `train_test_split` from a library called **scikit-learn** (or `sklearn` for short).  \n","Scikit-learn is one of the most widely used machine learning libraries in Python â€” it provides tools for building models, evaluating them, and managing data workflows."]},{"cell_type":"code","execution_count":2,"id":"ff2fd564","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ff2fd564","executionInfo":{"status":"ok","timestamp":1747954531607,"user_tz":240,"elapsed":3252,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"98ae4ed6-d8d3-41c6-b911-634f88b08b1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training messages: 4457\n","Test messages: 1115\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the dataset: 80% training, 20% testing\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Show the number of messages in each set\n","print(\"Training messages:\", len(train_df))\n","print(\"Test messages:\", len(test_df))"]},{"cell_type":"markdown","id":"5a5e7c62","metadata":{"id":"5a5e7c62"},"source":["The `random_state` parameter is like a seed for randomness â€” it ensures that we get the same split every time we run this code.\n","\n","Now weâ€™re ready to start building and training our Naive Bayes model using the `train_df` data!\n","\n","## Tokenizer\n","\n","Just like we did last class, weâ€™ll need a tokenizer. Weâ€™ll use the same function we defined earlier, which:\n","\n","- Converts text to lowercase\n","- Uses a regular expression to extract word-like tokens\n","- Removes stop words (common words like â€œtheâ€, â€œisâ€, â€œandâ€ that donâ€™t carry much meaning)\n","\n","Hereâ€™s the function:"]},{"cell_type":"code","execution_count":3,"id":"fc31ef41","metadata":{"id":"fc31ef41","executionInfo":{"status":"ok","timestamp":1747954531686,"user_tz":240,"elapsed":81,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import re\n","from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n","\n","def tokenize(text):\n","    lowercase_text = text.lower()\n","    tokens = re.findall(r'\\b\\w+\\b', lowercase_text)\n","    return [t for t in tokens if t not in ENGLISH_STOP_WORDS]"]},{"cell_type":"markdown","id":"94581e8a","metadata":{"id":"94581e8a"},"source":["## Create Word Frequency Tables\n","\n","Now that we can tokenize messages, weâ€™ll count how often each word appears in:\n","\n","- Spam messages\n","- Ham (non-spam) messages\n","\n","These counts will help us estimate the probabilities **P(word | spam)** and **P(word | ham)** â€” which the Naive Bayes model needs to make predictions.\n","\n","Weâ€™ll create two dictionaries:\n","\n","- `spam_counts` â€” maps each word to how many times it appears in spam messages\n","- `ham_counts` â€” same idea, but for ham messages"]},{"cell_type":"code","execution_count":4,"id":"53e3ceea","metadata":{"id":"53e3ceea","executionInfo":{"status":"ok","timestamp":1747954532306,"user_tz":240,"elapsed":619,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["# Initialize word count dictionaries\n","spam_counts = {}\n","ham_counts = {}\n","\n","# Loop through the training messages\n","for _, row in train_df.iterrows():\n","    tokens = tokenize(row[\"message\"])\n","    label = row[\"label\"]\n","\n","    for token in tokens:\n","        if label == \"spam\":\n","            spam_counts[token] = spam_counts.get(token, 0) + 1\n","        else:\n","            ham_counts[token] = ham_counts.get(token, 0) + 1"]},{"cell_type":"markdown","id":"6766a683","metadata":{"id":"6766a683"},"source":["At this point:\n","\n","- `spam_counts[\"win\"]` tells us how many times the word \"win\" appeared in spam messages\n","- `ham_counts[\"ok\"]` tells us how many times the word \"ok\" appeared in ham messages\n","\n","These raw counts are the building blocks of our model. Next, weâ€™ll convert them into **probabilities** â€” and apply **Laplace smoothing** to avoid zero-probability issues.\n","\n","## Estimate P(word | spam) and P(word | ham) with Laplace Smoothing\n","\n","Now that we have word frequency tables, we want to calculate:\n","\n","- **P(word | spam)** â€” how likely a word is to appear in spam messages\n","- **P(word | ham)** â€” how likely it is to appear in ham messages\n","\n","But there's a problem:  If a word never appears in one category (e.g., `\"pizza\"` never shows up in spam), then its probability is 0 â€” which would cause division by zero errors. To avoid this, we use **Laplace smoothing**.\n","\n","### Laplace Smoothing Formula\n","\n","To calculate smoothed probabilities, we use this formula:\n","\n","**P(word | class) = (count + 1) / (total_count + V)**\n","\n","Where:\n","- `count` = how many times the word appears in that class (spam or ham)\n","- `total_count` = total number of words in that class\n","- `V` = size of the full vocabulary (number of unique words across both classes)\n","\n","Adding `1` ensures that **every word has at least a tiny non-zero probability** in both classes."]},{"cell_type":"code","execution_count":5,"id":"79b232a5","metadata":{"id":"79b232a5","executionInfo":{"status":"ok","timestamp":1747954532308,"user_tz":240,"elapsed":17,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["# Combine vocab from both classes\n","vocab = set(spam_counts.keys()) | set(ham_counts.keys())\n","V = len(vocab)\n","\n","# Total number of words in each class\n","spam_total = sum(spam_counts.values())\n","ham_total = sum(ham_counts.values())\n","\n","# Create smoothed probability tables\n","P_word_given_spam = {}\n","P_word_given_ham = {}\n","\n","for word in vocab:\n","    # Smoothed spam probability\n","    spam_count = spam_counts.get(word, 0)\n","    P_word_given_spam[word] = (spam_count + 1) / (spam_total + V)\n","\n","    # Smoothed ham probability\n","    ham_count = ham_counts.get(word, 0)\n","    P_word_given_ham[word] = (ham_count + 1) / (ham_total + V)"]},{"cell_type":"markdown","id":"18ec3707","metadata":{"id":"18ec3707"},"source":["At this point:\n","\n","- `P_word_given_spam[\"win\"]` gives us the smoothed probability of the word \"win\" in spam\n","- `P_word_given_ham[\"ok\"]` gives us the smoothed probability of \"ok\" in ham\n","\n","These probabilities are now safe to use â€” even for rare or previously unseen words.\n","\n","Next, weâ€™ll convert these into **log probabilities** and build a function to classify new messages!\n","\n","## Convert to Log-Probabilities\n","\n","In Naive Bayes, we multiply together many small probabilities â€” one for each word in the message.  \n","But multiplying lots of small numbers can quickly underflow (i.e., become too small for the computer to handle).\n","\n","To fix that, we take the **logarithm** of each probability â€” which turns multiplication into addition:\n","\n","- `log(P(w1 | spam) Ã— P(w2 | spam))` â†’ `log(P(w1 | spam)) + log(P(w2 | spam))`\n","\n","This also makes our model behave more like the **lexicon-based model** from earlier â€”  \n","Each word contributes a score, and we just **add them up**.\n","\n","Weâ€™ll also precompute the **log-ratio** for each word:"]},{"cell_type":"code","execution_count":6,"id":"d046f29f","metadata":{"id":"d046f29f","executionInfo":{"status":"ok","timestamp":1747954532310,"user_tz":240,"elapsed":13,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import math\n","\n","log_ratios = {}\n","\n","for word in vocab:\n","    pw_spam = P_word_given_spam[word]\n","    pw_ham = P_word_given_ham[word]\n","    log_ratios[word] = math.log(pw_spam / pw_ham)"]},{"cell_type":"markdown","id":"38fcb683","metadata":{"id":"38fcb683"},"source":["This dictionary `log_ratios` is now our **\"spam lexicon\"**:\n","\n","- Words with positive values are more spammy\n","- Words with negative values are more ham-like\n","- Words with scores near zero donâ€™t strongly favor either class\n","\n","Now weâ€™re ready to build the final prediction function â€” using these word-level log scores!\n","\n","## Classify a Message with Naive Bayes\n","\n","To predict whether a message is spam or ham, we:\n","\n","1. Tokenize the message\n","2. For each word, look up its **log-ratio**: `log(P(word | spam) / P(word | ham))`\n","3. Add those values together\n","4. Add the **log prior ratio**: `log(P(spam) / P(ham))`\n","5. Predict **spam if the total score > 0**, otherwise **ham**\n","\n","Letâ€™s calculate the class priors from our training data:"]},{"cell_type":"code","execution_count":7,"id":"bb79f43b","metadata":{"id":"bb79f43b","executionInfo":{"status":"ok","timestamp":1747954532331,"user_tz":240,"elapsed":20,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["# Count spam and ham messages\n","num_spam = (train_df[\"label\"] == \"spam\").sum()\n","num_ham = (train_df[\"label\"] == \"ham\").sum()\n","\n","# Compute class prior probabilities\n","P_spam = num_spam / len(train_df)\n","P_ham = num_ham / len(train_df)\n","\n","# Compute log prior (the constant)\n","log_prior = math.log(P_spam / P_ham)"]},{"cell_type":"markdown","id":"4cae8bc9","metadata":{"id":"4cae8bc9"},"source":["Now weâ€™ll write a prediction function that:\n","\n","- Computes the total log-score of a message\n","- Returns `\"spam\"` if the score is greater than 0\n","- Returns `\"ham\"` otherwise"]},{"cell_type":"code","execution_count":8,"id":"6ae6fdbf","metadata":{"id":"6ae6fdbf","executionInfo":{"status":"ok","timestamp":1747954532370,"user_tz":240,"elapsed":32,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["def predict(message):\n","    tokens = tokenize(message)\n","    score = log_prior  # start with the class bias\n","\n","    for token in tokens:\n","        if token in log_ratios:\n","            score += log_ratios[token]\n","\n","    return \"spam\" if score > 0 else \"ham\""]},{"cell_type":"markdown","id":"c17f40ea","metadata":{"id":"c17f40ea"},"source":["Letâ€™s try it out on a few test messages!"]},{"cell_type":"code","execution_count":9,"id":"b9148d1e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9148d1e","executionInfo":{"status":"ok","timestamp":1747954532372,"user_tz":240,"elapsed":24,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"6db94bc4-c845-4524-829f-d173d9ee626e"},"outputs":[{"output_type":"stream","name":"stdout","text":["spam\n","ham\n"]}],"source":["print(predict(\"Congratulations! You've won a free ticket to Bahamas. Click here to claim.\"))\n","print(predict(\"Hey, are we still on for dinner tonight?\"))"]},{"cell_type":"markdown","id":"6a4e5ebf","metadata":{"id":"6a4e5ebf"},"source":["## Evaluate Model Accuracy on the Test Set\n","\n","Now that we have a working `predict()` function, weâ€™ll apply it to every message in the test set.\n","\n","Then weâ€™ll compare the predicted labels to the actual labels and calculate the modelâ€™s accuracy."]},{"cell_type":"code","execution_count":10,"id":"61d8f447","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61d8f447","executionInfo":{"status":"ok","timestamp":1747954532603,"user_tz":240,"elapsed":230,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"9ba17ddd-b34b-4fc7-e503-2390950b13ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 99.28%\n"]}],"source":["# Predict all messages in the test set\n","predictions = []\n","\n","for _, row in test_df.iterrows():\n","    message = row[\"message\"]\n","    prediction = predict(message)\n","    predictions.append(prediction)\n","\n","# Actual labels\n","actual = test_df[\"label\"].tolist()\n","\n","# Compute accuracy\n","correct = sum([pred == truth for pred, truth in zip(predictions, actual)])\n","accuracy = correct / len(test_df)\n","\n","print(f\"Accuracy: {accuracy:.2%}\")"]},{"cell_type":"markdown","id":"ada85a74","metadata":{"id":"ada85a74"},"source":["This tells us how well our Naive Bayes model generalizes to new data.\n","\n","In most cases, you should see accuracy well above **90%**, even with this simple bag-of-words approach.\n","\n","Next: we can analyze **which words had the strongest influence** on the model â€” or look at **false positives and false negatives** to better understand where it succeeds or fails.\n","\n","### Confusion Matrix\n","\n","A **confusion matrix** helps us see where the model gets things right and wrong:\n","\n","- **True Positives (TP)**: Spam correctly labeled as spam  \n","- **True Negatives (TN)**: Ham correctly labeled as ham  \n","- **False Positives (FP)**: Ham incorrectly labeled as spam  \n","- **False Negatives (FN)**: Spam incorrectly labeled as ham\n","\n","Weâ€™ll use `sklearn.metrics.confusion_matrix` to build one."]},{"cell_type":"code","execution_count":11,"id":"bb4ef5fb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bb4ef5fb","executionInfo":{"status":"ok","timestamp":1747954532628,"user_tz":240,"elapsed":27,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"75b812bf-e3fd-4eef-d7cf-2294934e5377"},"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n","               Predicted\n","             | spam | ham \n","Actual spam  |   143 |    6\n","Actual ham   |     2 |  964\n"]}],"source":["from sklearn.metrics import confusion_matrix\n","\n","# Generate confusion matrix\n","cm = confusion_matrix(actual, predictions, labels=[\"spam\", \"ham\"])\n","\n","# Display as a readable table\n","print(\"Confusion Matrix:\")\n","print(f\"               Predicted\")\n","print(f\"             | spam | ham \")\n","print(f\"Actual spam  |  {cm[0][0]:4} | {cm[0][1]:4}\")\n","print(f\"Actual ham   |  {cm[1][0]:4} | {cm[1][1]:4}\")"]},{"cell_type":"markdown","id":"ae3e533e","metadata":{"id":"ae3e533e"},"source":["**Why Accuracy Isnâ€™t Always Enough**\n","\n","Suppose 99% of the messages in your dataset are **ham**.  \n","A model that simply **guesses \"ham\" every time** would be right 99% of the time â€” and get **99% accuracy** â€” but it would be **completely useless** at detecting spam.\n","\n","This is why we need to look beyond accuracy and examine the **confusion matrix**:\n","- It shows **how many spam messages were missed** (false negatives)\n","- And **how many ham messages were mislabeled** as spam (false positives)\n","\n","These errors matter a lot â€” especially in real-world systems like email filtering or fraud detection, where the \"rare\" class is often the most important to catch.\n","\n","### Precision, Recall, and F1-Score\n","\n","When evaluating a classifier â€” especially on **imbalanced datasets** â€” it's important to go beyond accuracy.\n","\n","Letâ€™s define three important metrics:\n","\n","#### ðŸ”¹ Precision\n","\n","> Of all the messages the model predicted as **spam**, how many were actually spam?\n","\n","High precision means **few false positives**.\n","\n","#### ðŸ”¹ Recall\n","\n","> Of all the actual spam messages, how many did the model correctly identify?\n","\n","High recall means **few false negatives** â€” you're catching most of the spam.\n","\n","#### ðŸ”¹ F1-Score\n","\n","> A balanced average of precision and recall.\n","\n","F1 = 2 Ã— (precision Ã— recall) / (precision + recall)\n","\n","Useful when you care about both avoiding **false alarms** and **missing real spam**.\n","\n","We can compute all three using `sklearn.metrics.classification_report`:"]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(actual, predictions, target_names=[\"spam\", \"ham\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yr89vJSUPKr0","executionInfo":{"status":"ok","timestamp":1747955054241,"user_tz":240,"elapsed":49,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"f496ce73-fecc-48d7-fa21-a0d5108bdc27"},"id":"Yr89vJSUPKr0","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        spam       0.99      1.00      1.00       966\n","         ham       0.99      0.96      0.97       149\n","\n","    accuracy                           0.99      1115\n","   macro avg       0.99      0.98      0.98      1115\n","weighted avg       0.99      0.99      0.99      1115\n","\n"]}]},{"cell_type":"markdown","source":["This gives you a full breakdown of:\n","\n","- **Precision**: Of the messages predicted as spam/ham, how many were actually correct?\n","- **Recall**: Of the actual spam/ham messages, how many did the model correctly identify?\n","- **F1-score**: A balance between precision and recall â€” high only when both are high.\n","- **Support**: The number of true examples of each class in the test set.\n","- **Macro avg**: The unweighted average across classes â€” treats each class equally.\n","- **Weighted avg**: The average weighted by class frequency â€” reflects overall performance on imbalanced data.\n","\n","These metrics help you understand where your model is strong â€” and where it needs improvement. They are especially useful when the dataset has class imbalance (e.g., more ham than spam).\n","\n","### Most \"Spammy\" and \"Hammy\" Words\n","\n","Letâ€™s inspect which words had the strongest influence on our model â€” the highest and lowest log-ratios.\n","\n","Words with very positive log-ratios are strong indicators of spam.\n","\n","Words with very negative log-ratios are strong indicators of ham."],"metadata":{"id":"DMdq6be3PWBG"},"id":"DMdq6be3PWBG"},{"cell_type":"code","execution_count":12,"id":"40ac1414","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40ac1414","executionInfo":{"status":"ok","timestamp":1747954532661,"user_tz":240,"elapsed":32,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"7f305c17-6d09-4843-cc5e-f4b891760c8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ’¬ Most ham-like words:\n","gt              -4.7844\n","lt              -4.7804\n","Ã¼               -4.2375\n","lor             -4.0753\n","da              -4.0428\n","later           -3.9920\n","oh              -3.7688\n","doing           -3.5645\n","amp             -3.4955\n","said            -3.4955\n","\n","ðŸ’¬ Most spam-like words:\n","1000            4.2793\n","100             4.3083\n","cs              4.3365\n","500             4.3639\n","guaranteed      4.4665\n","18              4.4665\n","tone            4.6242\n","150p            4.7603\n","prize           4.9274\n","claim           5.2303\n"]}],"source":["# Sort words by log-ratio\n","sorted_words = sorted(log_ratios.items(), key=lambda x: x[1])\n","\n","# Most hammy (very negative log-ratio)\n","print(\"ðŸ’¬ Most ham-like words:\")\n","for word, score in sorted_words[:10]:\n","    print(f\"{word:15} {score:.4f}\")\n","\n","# Most spammy (very positive log-ratio)\n","print(\"\\nðŸ’¬ Most spam-like words:\")\n","for word, score in sorted_words[-10:]:\n","    print(f\"{word:15} {score:.4f}\")"]},{"cell_type":"markdown","id":"8700a6ab","metadata":{"id":"8700a6ab"},"source":["### Error Analysis\n","\n","Letâ€™s look at some misclassified messages â€” where the model's prediction didn't match the true label.\n","\n","This helps us understand:\n","\n","- Where the model is confused\n","- Whether certain types of spam are being missed\n","- If itâ€™s too aggressive (labeling ham as spam)"]},{"cell_type":"code","execution_count":13,"id":"2df17f70","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2df17f70","executionInfo":{"status":"ok","timestamp":1747954532695,"user_tz":240,"elapsed":4,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"dc6fd0f7-0d67-4ead-9540-071f8121ba14"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg Â£1.50\n","\n","--- Misclassified Message ---\n","Actual:    ham\n","Predicted: spam\n","Message:    what number do u live at? Is it 11?\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Babe: U want me dont u baby! Im nasty and have a thing 4 filthyguys. Fancy a rude time with a sexy bitch. How about we go slo n hard! Txt XXX SLO(4msgs)\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Hello darling how are you today? I would love to have a chat, why dont you tell me what you look like and what you are in to sexy?\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?\n","\n","--- Misclassified Message ---\n","Actual:    ham\n","Predicted: spam\n","Message:   How much would it cost to hire a hitman\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Sorry I missed your call let's talk when you have the time. I'm on 07090201529\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123\n"]}],"source":["# Show the first 10 misclassified messages\n","for i in range(len(test_df)):\n","    if predictions[i] != actual[i]:\n","        print(f\"\\n--- Misclassified Message ---\")\n","        print(f\"Actual:    {actual[i]}\")\n","        print(f\"Predicted: {predictions[i]}\")\n","        print(f\"Message:   {test_df.iloc[i]['message']}\")"]},{"cell_type":"markdown","source":["## Summary\n","\n","\n","This week, you learned how to:\n","\n","- **Compare rule-based and machine learningâ€“based systems** for text analysis  \n","- Distinguish between **supervised** and **unsupervised** learning  \n","- Use **Bayesâ€™ Rule** to build a Naive Bayes classifier for spam detection  \n","- Implement a **bag-of-words** model with **Laplace smoothing**  \n","- Convert word-level probabilities into **log-scores**  \n","- Classify new messages using a trained Naive Bayes model  \n","- Measure performance using **accuracy**, **confusion matrices**, and **precision/recall/F1**  \n","- Inspect **model behavior** by analyzing the most \"spammy\" and \"hammy\" words  \n","- Perform basic **error analysis** to see where the model makes mistakes  \n","\n","Even though Naive Bayes is a relatively simple algorithm, itâ€™s highly effective â€” and a great foundation for building intuition about how machine learning works with text.\n","\n","## Exercises\n","\n","1. **Manual Classification**  \n","   Pick 3 messages from the dataset (or your own inbox!).  \n","   Tokenize each one and estimate whether itâ€™s more likely spam or ham using your intuition and word-level log scores.\n","\n","2. **Evaluate with More Metrics**  \n","   Use `classification_report` to get the precision, recall, and F1-score for your model.  \n","   What do these scores tell you about your modelâ€™s strengths and weaknesses?\n","\n","3. **Inspect the Lexicon**  \n","   Print the 20 most \"spammy\" and \"hammy\" words from the `log_ratios` dictionary.  \n","   Are any of them surprising? Do they make sense?\n","\n","4. **Adjust the Prior**  \n","   What happens if you **manually increase or decrease** the prior (log(P(spam)/P(ham)))?  \n","   Try removing the prior entirely and observe how that affects performance on the test set.\n","\n","5. **Compare with Lexicon Model**  \n","   Conceptually compare the Naive Bayes model to the lexicon-based sentiment model from Week 2.  \n","   Whatâ€™s similar? Whatâ€™s different?\n","\n","6. **Error Inspection**  \n","   Use the error analysis loop to inspect at least 5 misclassified messages.  \n","   Try to identify *why* the model was confused.\n","\n","7. **Explore Further**  \n","   Try applying your Naive Bayes classifier to a **different dataset** â€” for example, Yelp reviews labeled as positive or negative.  \n","   What would change? What parts of your code could be reused?\n","\n","## Homework\n","\n","- Read chapters 16 thru 20 of [Think Python](https://allendowney.github.io/ThinkPython/)\n","- Complete the Exercises above\n","- Get going on your Group Project"],"metadata":{"id":"EeZ-EIKTSHVD"},"id":"EeZ-EIKTSHVD"}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}