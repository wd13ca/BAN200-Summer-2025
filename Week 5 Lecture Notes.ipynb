{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 5 Lecture Notes\n",
        "\n",
        "This week we will introduce neural networks. The vast majority of modern AI and NLP is done with neural nets, including larger language models like ChatGPT. We\u2019ll explore the building blocks of neural networks, from simple tensors to gradient descent, and begin constructing and training a simple model. By the end of this week, you'll understand the basics of how these models function, and start to understand how to implement them using Python.\n",
        "\n",
        "## What Is a Neural Network?\n",
        "\n",
        "A **neural network** is a type of machine learning model that can learn patterns from data to make predictions.\n",
        "\n",
        "Like the other models we've looked at, a neural network is really just a **mathematical equation**. It takes some **input** (represented as numbers), processes it through a series of steps, and produces some **output** (also represented as numbers).\n",
        "\n",
        "What makes neural networks different is **how they learn**. Instead of using:\n",
        "- Hard-coded rules (like we did with lexicon-based sentiment analysis)\n",
        "- Predefined probabilities based on counting (like we did with Naive Bayes)\n",
        "\n",
        "Neural networks learn their own internal rules using a method called **gradient descent** \u2014 a process that gradually adjusts the model\u2019s parameters to reduce error over time.\n",
        "\n",
        "### Why Are They Called Neural Networks?\n",
        "\n",
        "The neural networks we'll study in this course are more specifically called **artificial neural networks**, named after the **biological neural networks** found in animal brains.\n",
        "\n",
        "While the original idea was inspired by how real neurons fire and connect, artificial neural networks work quite differently. In practice, they are better thought of as **powerful pattern-matching systems** \u2014 mathematical models that learn from data, not true simulations of the brain.\n",
        "\n",
        "## Tensor\n",
        "\n",
        "Before we go further into neural networks, we need to talk about the data structures they use \u2014 specifically something called a **tensor**.\n",
        "\n",
        "### Tensors Are Just Generalized Arrays\n",
        "\n",
        "At a basic level, a **tensor** is just a container for numbers \u2014 kind of like a list, array, or table. In fact:\n",
        "\n",
        "- A **scalar** (a single number) is a **0D tensor**\n",
        "- A **vector** (a column or row of numbers) is a **1D tensor**\n",
        "- A **matrix** (a rectangle of numbers) is a **2D tensor**\n",
        "- A cube of numbers is a **3D tensor**\n",
        "\n",
        "Tensors can be extended into higher dimensions as well.\n",
        "\n",
        "### Input and Output Examples\n",
        "\n",
        "Tensors are used to represent **both inputs and outputs** in neural networks. Depending on the type of data we're working with, tensors can have different dimensions.\n",
        "\n",
        "Here are a few common examples:\n",
        "\n",
        "- **Regression output (0D tensor)**  \n",
        "  If your model predicts a single number (e.g., house price or sentiment score), the output is a scalar (0D tensor)\n",
        "\n",
        "- **Document input (1D tensor)**  \n",
        "  A document represented using bag-of-words or TF-IDF is a 1D tensor \u2014 a simple list of numbers\n",
        "\n",
        "- **Classification output (1D tensor)**  \n",
        "  In a classification model, the output can be represented as a 1D tensor with one element for each class  \n",
        "  For example, if you're building a model to route incoming customer emails into one of three categories \u2014 **Billing**, **Sales**, or **Tech Support** \u2014 the model might output a vector like:  \n",
        "  `[0.7, 0.1, 0.2]`  \n",
        "  These numbers can be interpreted as **probabilities**. In this case, the model is most confident the email is about a billing issue\n",
        "\n",
        "- **Black and white image input (2D tensor)**  \n",
        "  A grayscale image is stored as a 2D tensor with shape: height \u00d7 width\n",
        "\n",
        "- **Colour image input (3D tensor)**  \n",
        "  A colour image is stored as a 3D tensor with shape: height \u00d7 width \u00d7 colour channels  \n",
        "  For example, a 64\u00d764 RGB image would be represented as: 64 \u00d7 64 \u00d7 3\n",
        "\n",
        "- **Video input (4D tensor)**  \n",
        "  A short colour video can be represented as a 4D tensor with shape: time \u00d7 height \u00d7 width \u00d7 channels  \n",
        "  For example, a 10-frame video of 64\u00d764 RGB images: 10 \u00d7 64 \u00d7 64 \u00d7 3\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "A **linear regression** is one of the simplest types of neural networks. It\u2019s a model that learns to **predict a single number** (like a price, score, or value) by combining input features using a weighted sum.\n",
        "\n",
        "The mathematical form of a linear regression is:\n",
        "\n",
        "```\n",
        "\u0177 = w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b\n",
        "```\n",
        "\n",
        "- `x\u2081, x\u2082, ..., x\u2099` are the input features  \n",
        "- `w\u2081, w\u2082, ..., w\u2099` are the feature weights the model learns  \n",
        "- `b` is the bias weight (intercept) the model learns   \n",
        "- `\u0177` is the predicted output (a single number)\n",
        "\n",
        "This is also known as a **weighted sum** or a **linear combination**.\n",
        "\n",
        "### Example: Predicting a House Price\n",
        "\n",
        "Suppose we want to predict the price of a house based on three features:\n",
        "\n",
        "- `x\u2081 = 1200` (square footage)  \n",
        "- `x\u2082 = 3` (number of bedrooms)  \n",
        "- `x\u2083 = 1` (has garage: 1 = yes, 0 = no)\n",
        "\n",
        "Let\u2019s say our model has learned the following weights:\n",
        "\n",
        "- `w\u2081 = 150`  \n",
        "- `w\u2082 = 10,000`  \n",
        "- `w\u2083 = 5,000`  \n",
        "- `b = 20,000` (the base price)\n",
        "\n",
        "We plug these into the equation:\n",
        "\n",
        "```\n",
        "\u0177 = (150 \u00d7 1200) + (10,000 \u00d7 3) + (5,000 \u00d7 1) + 20,000  \n",
        "  = 180,000 + 30,000 + 5,000 + 20,000  \n",
        "  = 235,000\n",
        "```\n",
        "\n",
        "So, the model predicts that the house is worth **$235,000**.\n",
        "\n",
        "### Vector Form\n",
        "\n",
        "You can also write this equation in **vector form**:\n",
        "\n",
        "```\n",
        "\u0177 = w \u00b7 x + b\n",
        "```\n",
        "\n",
        "- `w` is a **1D tensor** (vector) of weights  \n",
        "- `x` is a **1D tensor** (vector) of input features  \n",
        "- `\u00b7` represents the **dot product**, which multiplies each pair of values and sums the results  \n",
        "- `b` is still the bias  \n",
        "- `\u0177` is the output \u2014 a single number\n",
        "\n",
        "This vectorized version does exactly the same thing: it multiplies each input by its corresponding weight, adds them all together, and then adds the bias.\n",
        "\n",
        "It\u2019s mathematically identical to the expanded version \u2014 just more compact and efficient, especially when working with large models or datasets.\n",
        "\n",
        "## Loss Functions\n",
        "\n",
        "So far, we've talked about how a linear regression model makes predictions using weights (`w`) and a bias (`b`). But how do we know if a particular set of weights is **good or bad**?\n",
        "\n",
        "To train a model, we need to define what it means for a model to perform well. This is where **loss functions** come in.\n",
        "\n",
        "### What Are We Trying to Do?\n",
        "\n",
        "At the end of the day, we want to find weights and a bias that make the model's predictions as accurate as possible. That means we need a way to **measure how far off the predictions are** from the actual answers.\n",
        "\n",
        "That\u2019s the job of a **loss function**: it assigns a number (the **loss**) to each prediction, telling us how bad it was. Lower is better.\n",
        "\n",
        "> **Key idea:**  \n",
        "> A **loss function** tells us how well the model is doing.  \n",
        "> We want to find the weights (`w`) and bias (`b`) that **minimize the loss**.\n",
        "\n",
        "### Mean Squared Error (MSE)\n",
        "\n",
        "For regression tasks (predicting a number), a common loss function is **Mean Squared Error (MSE)**:\n",
        "\n",
        "```\n",
        "MSE = average((\u0177 - y)\u00b2)\n",
        "```\n",
        "\n",
        "- `\u0177` is the predicted value from the model  \n",
        "- `y` is the true value from the training data  \n",
        "- We subtract them to get the **error**, then square it (to make all errors positive and penalize bigger mistakes more)\n",
        "\n",
        "#### Example:\n",
        "\n",
        "If the true house price is `$300,000` and the model predicts `$280,000`, the squared error is:\n",
        "\n",
        "```\n",
        "(280,000 - 300,000)\u00b2 = 400,000,000\n",
        "```\n",
        "\n",
        "If we do this for every house in the dataset and take the average, we get the MSE \u2014 a single number that summarizes how far off our predictions are.\n",
        "\n",
        "### Why Not Just Take the Mean Error?\n",
        "\n",
        "You might wonder: why not just compute the **mean error** like this?\n",
        "\n",
        "```\n",
        "average(\u0177 - y)\n",
        "```\n",
        "\n",
        "This won\u2019t work \u2014 because **positive and negative errors will cancel each other out**.  \n",
        "\n",
        "For example, if the model overpredicts one example by 20 and underpredicts another by 20, the average error would be 0 \u2014 even though both predictions were wrong.\n",
        "\n",
        "To avoid this cancellation, we square each error. This makes all the errors positive and ensures that **larger mistakes are penalized more heavily**.\n",
        "\n",
        "### Why Not Just Take the Mean Absolute Error?\n",
        "\n",
        "Another option is to use **Mean Absolute Error (MAE)**:\n",
        "\n",
        "```\n",
        "MAE = average(|\u0177 - y|)\n",
        "```\n",
        "\n",
        "Instead of squaring the error, we take the **absolute value** \u2014 which also avoids cancellation and gives a clear measure of the average size of the errors.\n",
        "\n",
        "So why don\u2019t we use this as the default?\n",
        "\n",
        "#### A Historical Reason: It Was Too Hard to Calculate\n",
        "\n",
        "When linear regression was first developed in the early 1800s \u2014 long before calculators or computers \u2014 all the math had to be done **by hand**.  \n",
        "\n",
        "And it turns out that **minimizing absolute error is much harder to do by hand** than minimizing squared error.\n",
        "\n",
        "So squared error became the standard \u2014 not because it was the only option, but because it was the one that was **mathematically convenient** at the time. And it stuck.\n",
        "\n",
        "That said, today **mean absolute error** is often used instead of MSE.\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "Now that we know how to measure how \"good\" a set of weights is \u2014 using a **loss function** \u2014 the next question is:\n",
        "\n",
        "> **How do we actually find good weights?**\n",
        "\n",
        "There are infinitely many possible values for the weights and bias. We can't try them all, and there's no obvious formula that tells us the best ones.  \n",
        "So instead of guessing randomly or exhaustively searching, we need an **intelligent way to search** for a good solution.\n",
        "\n",
        "That strategy is called **gradient descent**.\n",
        "\n",
        "### The Basic Idea\n",
        "\n",
        "Imagine you're standing somewhere on a hilly landscape, and your goal is to get to the bottom of the valley \u2014 where the **loss** is lowest.\n",
        "\n",
        "You can\u2019t see the whole terrain, but you can feel which direction the ground slopes. So you take a step downhill. Then another. Then another.  \n",
        "Eventually, you get closer and closer to the lowest point.\n",
        "\n",
        "This is exactly what **gradient descent** does \u2014 but instead of navigating a landscape, it\u2019s searching through possible weights for your model.  \n",
        "It moves step-by-step in the direction that **reduces the loss**.\n",
        "\n",
        "> **Gradient descent is an optimization algorithm that tweaks the model\u2019s weights to reduce the loss.**\n",
        "\n",
        "### How It Works\n",
        "\n",
        "At each training step:\n",
        "\n",
        "1. The model makes a prediction using the current weights and bias\n",
        "2. The prediction is compared to the true answer using the **loss function**\n",
        "3. The **gradient** of the loss is computed \u2014 how much the loss would change if we nudged each weight a little\n",
        "4. The weights are updated by taking a small step in the direction that reduces the loss\n",
        "\n",
        "This step is done for **each weight** and the bias.\n",
        "\n",
        "#### Update Rule\n",
        "\n",
        "The standard update formula looks like this:\n",
        "\n",
        "```\n",
        "w\u1d62 \u2190 w\u1d62 - \u03b7 * \u2202L/\u2202w\u1d62\n",
        "```\n",
        "\n",
        "- `w\u1d62` is the current weight  \n",
        "- `\u2202L/\u2202w\u1d62` is the gradient of the loss with respect to that weight  \n",
        "- `\u03b7` (eta) is the **learning rate** \u2014 a small constant that controls the step size  \n",
        "- The new weight is the old weight **minus** a small step in the direction of the gradient\n",
        "\n",
        "We do the same for the bias `b`:\n",
        "\n",
        "```\n",
        "b \u2190 b - \u03b7 * \u2202L/\u2202b\n",
        "```\n",
        "\n",
        "> The minus sign is important \u2014 it tells us to move **downhill**, not uphill.\n",
        "\n",
        "### The Learning Rate\n",
        "\n",
        "The learning rate `\u03b7` is a small number like `0.01` or `0.001`. It controls **how big each step is**.\n",
        "\n",
        "- If it's too small, training will be slow  \n",
        "- If it's too big, training can overshoot the minimum and never settle\n",
        "\n",
        "Finding a good learning rate is part of tuning the training process.\n",
        "\n",
        "### A Simple Example\n",
        "\n",
        "Let\u2019s say we have a model with just one weight:\n",
        "\n",
        "- The current weight is `w = 2.0`  \n",
        "- The loss function tells us that increasing `w` increases the loss  \n",
        "- The gradient is `\u2202L/\u2202w = 4.0`  \n",
        "- We\u2019re using a learning rate of `\u03b7 = 0.1`\n",
        "\n",
        "We update the weight:\n",
        "\n",
        "```\n",
        "w \u2190 2.0 - 0.1 * 4.0 = 1.6\n",
        "```\n",
        "\n",
        "We\u2019ve taken a small step **toward lower loss**.\n",
        "\n",
        "On the next step, we repeat the process \u2014 compute a new prediction, get the new loss and gradient, and update again.  \n",
        "This process continues until the loss stops improving (or we reach a set number of steps).\n",
        "\n",
        "### Why It Works\n",
        "\n",
        "The magic of gradient descent is that it **uses the slope of the loss function** to guide the search.\n",
        "\n",
        "- If the slope is steep, the weight update is bigger  \n",
        "- If we\u2019re close to the minimum, the slope flattens, and the updates become smaller  \n",
        "- Over time, the model settles into a **minimum-loss** solution\n",
        "\n",
        "> **Key idea:**  \n",
        "> Gradient descent turns learning into a process of repeated small improvements,  \n",
        "> guided by math, not guesswork.\n",
        "\n",
        "### Will It Always Find the Best Solution?\n",
        "\n",
        "Not necessarily.\n",
        "\n",
        "Gradient descent is a powerful tool, but it doesn\u2019t **guarantee** that we\u2019ll find the best possible set of weights \u2014 the true global minimum of the loss function.\n",
        "\n",
        "Why not?\n",
        "\n",
        "- The **loss landscape** can be complicated, especially for deep neural networks\n",
        "- It might have **multiple valleys** (local minima), **flat regions**, or **noisy slopes**\n",
        "- Gradient descent can get stuck in a **local minimum**, or just settle somewhere \u201cgood enough\u201d\n",
        "\n",
        "In practice, this usually isn\u2019t a big problem. For many real-world tasks, getting to a **low-loss** solution (even if it's not the lowest possible) is good enough \u2014 and gradient descent gets us there surprisingly well.\n",
        "\n",
        "> So while gradient descent doesn\u2019t always find *the best*, it often finds something **very useful** \u2014 and it does it fast.\n",
        "\n",
        "### Batch Gradient Descent\n",
        "\n",
        "When we train a neural network using gradient descent, we need to compute how wrong the model is \u2014 the **loss** \u2014 and then use that to update the weights.\n",
        "\n",
        "But here\u2019s an important question:\n",
        "\n",
        "> **How much data should we use to compute that update?**\n",
        "\n",
        "There are actually a few different strategies, and they each have tradeoffs.\n",
        "\n",
        "#### 1. Full-Batch Gradient Descent\n",
        "\n",
        "This is the most straightforward idea:\n",
        "\n",
        "- Use the **entire training dataset** to compute the average loss  \n",
        "- Then update the weights once, using the overall gradient\n",
        "\n",
        "\n",
        "#### 2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "At the opposite extreme, we can update the model using **just one example at a time**:\n",
        "\n",
        "- Pick one training example  \n",
        "- Compute the loss and gradient  \n",
        "- Update the weights  \n",
        "- Repeat\n",
        "\n",
        "This is called **stochastic gradient descent** \u2014 \"stochastic\" meaning random. It updates the weights using **noisy**, fast updates.\n",
        "\n",
        "#### 3. Mini-Batch Gradient Descent (the most common)\n",
        "\n",
        "Stochastic Gradient Descent (SGD) updates the model using one training example at a time, which makes each update very fast. This can be a big advantage when working with large datasets. Because it's based on just a single example, SGD introduces some randomness (\"noise\") into the updates \u2014 which can help the model escape local minimums but also makes training less stable. Full-batch gradient descent, on the other hand, uses the entire dataset to compute each update. This gives a very accurate gradient and makes learning more stable and predictable \u2014 but each update is much slower, and it requires a lot more memory. In practice, most modern training uses a compromise: **mini-batch gradient descent**, which combines the speed of SGD with the stability of full-batch.\n",
        "\n",
        "> **Mini-batch gradient descent** uses a small number of examples at a time \u2014 usually 32, 64, or 128.\n",
        "\n",
        "Each step works like this:\n",
        "\n",
        "- Pick a small **batch** of examples (e.g. 64 messages)\n",
        "- Compute the average loss and gradient for just that batch\n",
        "- Update the weights\n",
        "- Move to the next batch\n",
        "\n",
        "### Epochs\n",
        "\n",
        "When we say we\u2019ve done one **epoch** of training, it means we\u2019ve gone through the **entire training dataset once**. Training usually takes **multiple epochs**, so the model gets many chances to improve. Over time, the weights are refined to make better and better predictions.\n",
        "\n",
        "\n",
        "## Deriving the Gradient of MSE Loss for Linear Regression\n",
        "\n",
        "Let\u2019s derive the gradient we need for training a linear regression model using gradient descent.\n",
        "\n",
        "### Step 1: The Model\n",
        "\n",
        "We start with the linear regression equation:\n",
        "\n",
        "**\u0177 = w \u00b7 x + b**\n",
        "\n",
        "Where:  \n",
        "- `x` is the input vector (a 1D tensor of features)  \n",
        "- `w` is the weight vector (same size as `x`)  \n",
        "- `b` is the bias (a scalar)  \n",
        "- `\u0177` is the predicted output (a scalar)\n",
        "\n",
        "### Step 2: The Loss Function\n",
        "\n",
        "We\u2019ll use **Mean Squared Error (MSE)** as our loss function.  \n",
        "For a single training example:\n",
        "\n",
        "**L = (\u0177 - y)\u00b2**\n",
        "\n",
        "Where:  \n",
        "- `\u0177 = w \u00b7 x + b` is the predicted value  \n",
        "- `y` is the true value from the training data\n",
        "\n",
        "We want to compute the gradients of `L` with respect to `w` and `b`.\n",
        "\n",
        "### Step 3: Gradient with Respect to Weights (w)\n",
        "\n",
        "We use the chain rule:\n",
        "\n",
        "**dL/dw = dL/d\u0177 \u00b7 d\u0177/dw**\n",
        "\n",
        "First:\n",
        "\n",
        "**dL/d\u0177 = 2(\u0177 - y)**\n",
        "\n",
        "Then:\n",
        "\n",
        "**d\u0177/dw = x**\n",
        "\n",
        "Putting it together:\n",
        "\n",
        "**dL/dw = 2(\u0177 - y) \u00b7 x**\n",
        "\n",
        "### Step 4: Gradient with Respect to Bias (b)\n",
        "\n",
        "Again, using the chain rule:\n",
        "\n",
        "**dL/db = dL/d\u0177 \u00b7 d\u0177/db**\n",
        "\n",
        "We already have:\n",
        "\n",
        "**dL/d\u0177 = 2(\u0177 - y)**\n",
        "\n",
        "And:\n",
        "\n",
        "**d\u0177/db = 1**\n",
        "\n",
        "So:\n",
        "\n",
        "**dL/db = 2(\u0177 - y)**\n",
        "\n",
        "### Final Gradient Equations\n",
        "\n",
        "For one training example:\n",
        "\n",
        "- **Gradient of the weights:**  \n",
        "  **\u2207w L = 2(\u0177 - y) \u00b7 x**\n",
        "\n",
        "- **Gradient of the bias:**  \n",
        "  **\u2207b L = 2(\u0177 - y)**\n",
        "\n",
        "These gradients tell us how to update the weights and bias in the direction that reduces the loss.\n",
        "\n",
        "### Note on Averaging (for Mini-Batches)\n",
        "\n",
        "If you're working with a **batch of examples**, you would average the gradients over the batch:\n",
        "\n",
        "- **\u2207w L = average over batch of [2(\u0177 - y) \u00b7 x]**  \n",
        "- **\u2207b L = average over batch of [2(\u0177 - y)]**\n",
        "\n",
        "This ensures that the update reflects the overall trend across the examples, not just a single case.\n",
        "\n",
        "## Predicting Ice Cream Sales\n",
        "\n",
        "Imagine you run an **ice cream shop**, and you\u2019ve recorded your **daily ice cream sales** alongside the **temperature** on that day.\n",
        "\n",
        "We want to build a model that can predict **ice cream sales (`y`)** based on the **temperature (`x`)**.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We'll use a simple **synthetic dataset** that simulates our real-world scenario. The dataset is generated using this rough formula:\n",
        "\n",
        "```\n",
        "y \u2248 126 + 13x + noise\n",
        "```\n",
        "\n",
        "- `x` is the temperature (in degrees Celsius), randomly sampled between 0 and 30  \n",
        "- `y` is the number of ice cream cones sold, with some random noise added to simulate real-world variability  \n",
        "- The noise makes the relationship more realistic \u2014 it's not perfectly linear, just mostly linear\n",
        "\n",
        "This synthetic data allows us to:\n",
        "- Focus on the **core ideas** of linear regression\n",
        "- Keep the math and code simple\n",
        "- Easily **visualize** the data and the model, since it has only one feature\n",
        "\n",
        "We\u2019ve wrapped the dataset in a small Python class called `IceCreamDataSet` that lets us generate and plot the data easily.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(13)\n",
        "\n",
        "class IceCreamDataSet:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = 100\n",
        "    self.x = [random.uniform(0,30) for _ in range(self.n)]\n",
        "    self.y = [round(max(0,126 + 13*xi + random.normalvariate(0,26))) for xi in self.x]\n",
        "\n",
        "  def plot_data(self):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(self.x,self.y)\n",
        "    plt.xlabel(\"Temperature (\u00b0C)\")\n",
        "    plt.ylabel(\"Ice Cream Sales\")\n",
        "    plt.title(\"Ice Cream Sales vs. Temperature\")\n",
        "    plt.show()\n",
        "\n",
        "  def plot_model(self,model):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(self.x,self.y)\n",
        "    line_x = [min(self.x), max(self.x)]\n",
        "    line_y = [model.forward(xi) for xi in line_x]\n",
        "    plt.plot(line_x,line_y,color='red')\n",
        "    plt.xlabel(\"Temperature (\u00b0C)\")\n",
        "    plt.ylabel(\"Ice Cream Sales\")\n",
        "    plt.title(\"Model Prediction vs. Data\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Here, we create an instance of the synthetic dataset, simulating 100 days of ice cream sales based on daily temperature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = IceCreamDataSet()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let's show a scatterplot of the data, where each point represents one day's temperature and the corresponding number of ice cream cones sold.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.plot_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Building a Simple Linear Regression Model\n",
        "\n",
        "Now that we have a dataset, we need a model to learn from it.\n",
        "\n",
        "We\u2019ll create a simple **linear regression model** from scratch. The model will predict ice cream sales using this formula:\n",
        "\n",
        "```\n",
        "\u0177 = w * x + b\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "- `x` is the input (temperature)\n",
        "- `\u0177` is the predicted output (sales)\n",
        "- `w` is the **weight** (slope), and `b` is the **bias** (intercept) \u2014 both are parameters the model will learn\n",
        "\n",
        "Here\u2019s a class that defines the model and its two core operations:\n",
        "\n",
        "- `forward`: Makes a prediction\n",
        "- `backward`: Computes gradients of the loss with respect to `w` and `b`, using Mean Squared Error (MSE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearModel:\n",
        "    \"\"\"\n",
        "    A simple linear regression model:\n",
        "    \u0177 = w * x + b\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize weights and bias to zero\n",
        "        self.w = 0\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Predict the output \u0177 given input x.\n",
        "        \"\"\"\n",
        "        return self.w * x + self.b\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute gradients of the loss with respect to w and b\n",
        "        using Mean Squared Error (MSE) for a single example.\n",
        "        \"\"\"\n",
        "        y_pred = self.forward(x)\n",
        "        error = y_pred - y\n",
        "        dw = 2 * error * x   # gradient w.r.t. w\n",
        "        db = 2 * error       # gradient w.r.t. b\n",
        "        return dw, db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let's use the class to create a model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LinearModel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let's take a look at the initial weights:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.plot_model(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Why Is It Called \"Backward\"?\n",
        "\n",
        "The function that computes the gradient is called `backward` because it\u2019s about working **backward from the error** to figure out how to adjust the model.\n",
        "\n",
        "In the **forward** step, the model takes an input (like temperature), runs it through the equation (`\u0177 = w * x + b`), and produces a prediction.  \n",
        "\n",
        "Then we compare that prediction to the true answer and calculate how wrong it was \u2014 the **loss**.\n",
        "\n",
        "In the **backward** step, we take that loss and ask:  \n",
        "> *How should we change the weights and bias to reduce this error next time?*\n",
        "\n",
        "To answer that, we work backward through the equation, computing how much each part (each parameter) contributed to the mistake. This is the start of a process called **backpropagation** \u2014 short for *backward propagation of errors*.\n",
        "\n",
        "In more complex networks, this process involves the **chain rule** from calculus, which allows us to pass gradients backward through layers of functions.  \n",
        "We\u2019ll see that later \u2014 but even now, you\u2019re seeing the core idea:  \n",
        "> *To learn, a model must look at the error and trace it back to the parts that caused it.*\n",
        "\n",
        "### Training the Model (One Example at a Time)\n",
        "\n",
        "Now that we have a model and a dataset, let\u2019s train the model using **gradient descent**.\n",
        "\n",
        "We'll use a very simple form of training called **stochastic gradient descent**. That means we\u2019ll update the model **one example at a time** \u2014 using just a single `(x, y)` pair per update.\n",
        "\n",
        "For each training example:\n",
        "\n",
        "1. Make a prediction using the model  \n",
        "2. Compare it to the actual value to get the error  \n",
        "3. Use the `backward()` method to compute the gradients  \n",
        "4. Adjust the model\u2019s parameters (`w` and `b`) to reduce the error\n",
        "\n",
        "To visualize the training process, we\u2019ll also plot the model\u2019s current line after each epoch, and pause briefly so we can see it evolve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Set the learning rate \u2014 controls how big each update is\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Train for 100 passes (epochs) over the dataset\n",
        "for epoch in range(100):\n",
        "    for i in range(dataset.n):\n",
        "        # Get one example (temperature and sales)\n",
        "        x = dataset.x[i]\n",
        "        y = dataset.y[i]\n",
        "\n",
        "        # Forward pass: make a prediction\n",
        "        y_pred = model.forward(x)\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        dw, db = model.backward(x, y)\n",
        "\n",
        "        # Update weights and bias using gradient descent\n",
        "        model.w -= learning_rate * dw\n",
        "        model.b -= learning_rate * db\n",
        "\n",
        "    # Clear previous plot/output\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Print current progress\n",
        "    print(\"Epoch:\", epoch)\n",
        "    print(\"Learned weight (w):\", model.w)\n",
        "    print(\"Learned bias (b):\", model.b)\n",
        "\n",
        "    # Plot the current model line\n",
        "    dataset.plot_model(model)\n",
        "\n",
        "    # Pause so we can see the update before moving on\n",
        "    time.sleep(0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Group Project Reminder\n",
        "\n",
        "Each group must meet with me to discuss their Project Proposal. Meetings will take place during the Week 7 class. Not all members of the group need to attend. If no one from your group is able to attend during class time, please email me by June 8 to make an accommodation.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}