{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185ab978",
   "metadata": {},
   "source": [
    "# Week 5 Lecture Notes\n",
    "\n",
    "This week, we introduce one of the most powerful families of models in modern machine learning: **neural networks**. We’ll start by building intuition around how neural networks work, why they’re called \"neural\", and how they learn from data. Along the way, we’ll revisit linear regression from a new perspective, explore the mathematics of learning via **gradient descent**, and extend our toolkit to include **logistic regression for classification**. By the end of this lecture, you’ll see how these foundational ideas set the stage for more advanced models like deep learning.\n",
    "\n",
    "\n",
    "## What Is a Neural Network?\n",
    "\n",
    "A **neural network** is a type of machine learning model that can learn patterns from data to make predictions.\n",
    "\n",
    "Like the other models we've looked at, a neural network is really just a **mathematical equation**. It takes some **input** (represented as numbers), processes it through a series of steps, and produces some **output** (also represented as numbers).\n",
    "\n",
    "What makes neural networks different is **how they learn**. Instead of using:\n",
    "- Hard-coded rules (like we did with lexicon-based sentiment analysis)\n",
    "- Predefined probabilities based on counting (like we did with Naive Bayes)\n",
    "\n",
    "Neural networks learn their own internal rules using a method called **gradient descent** — a process that gradually adjusts the model’s parameters to reduce error over time.\n",
    "\n",
    "### Why Are They Called Neural Networks?\n",
    "\n",
    "The neural networks we'll study in this course are more specifically called **artificial neural networks**, named after the **biological neural networks** found in animal brains.\n",
    "\n",
    "While the original idea was inspired by how real neurons fire and connect, artificial neural networks work quite differently. In practice, they are better thought of as **powerful pattern-matching systems** — mathematical models that learn from data, not true simulations of the brain.\n",
    "\n",
    "## Tensor\n",
    "\n",
    "Before we go further into neural networks, we need to talk about the data structures they use — specifically something called a **tensor**.\n",
    "\n",
    "### Tensors Are Just Generalized Arrays\n",
    "\n",
    "At a basic level, a **tensor** is just a container for numbers — kind of like a list, array, or table. In fact:\n",
    "\n",
    "- A **scalar** (a single number) is a **0D tensor**\n",
    "- A **vector** (a column or row of numbers) is a **1D tensor**\n",
    "- A **matrix** (a rectangle of numbers) is a **2D tensor**\n",
    "- A cube of numbers is a **3D tensor**\n",
    "\n",
    "Tensors can be extended into higher dimensions as well.\n",
    "\n",
    "### Input and Output Examples\n",
    "\n",
    "Tensors are used to represent **both inputs and outputs** in neural networks. Depending on the type of data we're working with, tensors can have different dimensions.\n",
    "\n",
    "Here are a few common examples:\n",
    "\n",
    "- **Regression output (0D tensor)**  \n",
    "  If your model predicts a single number (e.g., house price or sentiment score), the output is a scalar (0D tensor)\n",
    "\n",
    "- **Document input (1D tensor)**  \n",
    "  A document represented using bag-of-words or TF-IDF is a 1D tensor — a simple list of numbers\n",
    "\n",
    "- **Classification output (1D tensor)**  \n",
    "  In a classification model, the output can be represented as a 1D tensor with one element for each class  \n",
    "  For example, if you're building a model to route incoming customer emails into one of three categories — **Billing**, **Sales**, or **Tech Support** — the model might output a vector like:  \n",
    "  `[0.7, 0.1, 0.2]`  \n",
    "  These numbers can be interpreted as **probabilities**. In this case, the model is most confident the email is about a billing issue\n",
    "\n",
    "- **Black and white image input (2D tensor)**  \n",
    "  A grayscale image is stored as a 2D tensor with shape: height × width\n",
    "\n",
    "- **Colour image input (3D tensor)**  \n",
    "  A colour image is stored as a 3D tensor with shape: height × width × colour channels  \n",
    "  For example, a 64×64 RGB image would be represented as: 64 × 64 × 3\n",
    "\n",
    "- **Video input (4D tensor)**  \n",
    "  A short colour video can be represented as a 4D tensor with shape: time × height × width × channels  \n",
    "  For example, a 10-frame video of 64×64 RGB images: 10 × 64 × 64 × 3\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "A **linear regression** is one of the simplest types of neural networks. It’s a model that learns to **predict a single number** (like a price, score, or value) by combining input features using a weighted sum.\n",
    "\n",
    "The mathematical form of a linear regression is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9568fb",
   "metadata": {},
   "source": [
    "```None\n",
    "ŷ = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740a0dc",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "- `x₁, x₂, ..., xₙ` are the input features  \n",
    "- `w₁, w₂, ..., wₙ` are the feature weights the model learns  \n",
    "- `b` is the bias weight (intercept) the model learns   \n",
    "- `ŷ` is the predicted output (a single number)\n",
    "\n",
    "This is also known as a **weighted sum** or a **linear combination**.\n",
    "\n",
    "### Example: Predicting a House Price\n",
    "\n",
    "Suppose we want to predict the price of a house based on three features:\n",
    "\n",
    "- `x₁ = 1200` (square footage)  \n",
    "- `x₂ = 3` (number of bedrooms)  \n",
    "- `x₃ = 1` (has garage: 1 = yes, 0 = no)\n",
    "\n",
    "Let’s say our model has learned the following weights:\n",
    "\n",
    "- `w₁ = 150`  \n",
    "- `w₂ = 10,000`  \n",
    "- `w₃ = 5,000`  \n",
    "- `b = 20,000` (the base price)\n",
    "\n",
    "We plug these into the equation:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6458f1",
   "metadata": {},
   "source": [
    "```ŷ = (150 × 1200) + (10,000 × 3) + (5,000 × 1) + 20,000  \n",
    "  = 180,000 + 30,000 + 5,000 + 20,000  \n",
    "  = 235,000\n",
    "\n",
    "\n",
    "\n",
    "So, the model predicts that the house is worth **$235,000**.\n",
    "\n",
    "### Vector Form\n",
    "\n",
    "You can also write this equation in **vector form**:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bcbbb",
   "metadata": {},
   "source": [
    "```None\n",
    "ŷ = w · x + b\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb080eb",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "- `w` is a **1D tensor** (vector) of weights  \n",
    "- `x` is a **1D tensor** (vector) of input features  \n",
    "- `·` represents the **dot product**, which multiplies each pair of values and sums the results  \n",
    "- `b` is still the bias  \n",
    "- `ŷ` is the output — a single number\n",
    "\n",
    "This vectorized version does exactly the same thing: it multiplies each input by its corresponding weight, adds them all together, and then adds the bias.\n",
    "\n",
    "It’s mathematically identical to the expanded version — just more compact and efficient, especially when working with large models or datasets.\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "So far, we've talked about how a linear regression model makes predictions using weights (`w`) and a bias (`b`). But how do we know if a particular set of weights is **good or bad**?\n",
    "\n",
    "To train a model, we need to define what it means for a model to perform well. This is where **loss functions** come in.\n",
    "\n",
    "### What Are We Trying to Do?\n",
    "\n",
    "At the end of the day, we want to find weights and a bias that make the model's predictions as accurate as possible. That means we need a way to **measure how far off the predictions are** from the actual answers.\n",
    "\n",
    "That’s the job of a **loss function**: it assigns a number (the **loss**) to each prediction, telling us how bad it was. Lower is better.\n",
    "\n",
    "> **Key idea:**  \n",
    "> A **loss function** tells us how well the model is doing.  \n",
    "> We want to find the weights (`w`) and bias (`b`) that **minimize the loss**.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "For regression tasks (predicting a number), a common loss function is **Mean Squared Error (MSE)**:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52e46f",
   "metadata": {},
   "source": [
    "```MSE = average((ŷ - y)²)\n",
    "\n",
    "\n",
    "\n",
    "- `ŷ` is the predicted value from the model  \n",
    "- `y` is the true value from the training data  \n",
    "- We subtract them to get the **error**, then square it (to make all errors positive and penalize bigger mistakes more)\n",
    "\n",
    "#### Example:\n",
    "\n",
    "If the true house price is `$300,000` and the model predicts `$280,000`, the squared error is:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efdbc8",
   "metadata": {},
   "source": [
    "```None\n",
    "(280,000 - 300,000)² = 400,000,000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe127f1",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "If we do this for every house in the dataset and take the average, we get the MSE — a single number that summarizes how far off our predictions are.\n",
    "\n",
    "### Why Not Just Take the Mean Error?\n",
    "\n",
    "You might wonder: why not just compute the **mean error** like this?\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5283c",
   "metadata": {},
   "source": [
    "```average(ŷ - y)\n",
    "\n",
    "\n",
    "\n",
    "This won’t work — because **positive and negative errors will cancel each other out**.  \n",
    "\n",
    "For example, if the model overpredicts one example by 20 and underpredicts another by 20, the average error would be 0 — even though both predictions were wrong.\n",
    "\n",
    "To avoid this cancellation, we square each error. This makes all the errors positive and ensures that **larger mistakes are penalized more heavily**.\n",
    "\n",
    "### Why Not Just Take the Mean Absolute Error?\n",
    "\n",
    "Another option is to use **Mean Absolute Error (MAE)**:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40808e27",
   "metadata": {},
   "source": [
    "```None\n",
    "MAE = average(|ŷ - y|)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdbf2b",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Instead of squaring the error, we take the **absolute value** — which also avoids cancellation and gives a clear measure of the average size of the errors.\n",
    "\n",
    "So why don’t we use this as the default?\n",
    "\n",
    "#### A Historical Reason: It Was Too Hard to Calculate\n",
    "\n",
    "When linear regression was first developed in the early 1800s — long before calculators or computers — all the math had to be done **by hand**.  \n",
    "\n",
    "And it turns out that **minimizing absolute error is much harder to do by hand** than minimizing squared error.\n",
    "\n",
    "So squared error became the standard — not because it was the only option, but because it was the one that was **mathematically convenient** at the time. And it stuck.\n",
    "\n",
    "That said, today **mean absolute error** is often used instead of MSE.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Now that we know how to measure how \"good\" a set of weights is — using a **loss function** — the next question is:\n",
    "\n",
    "> **How do we actually find good weights?**\n",
    "\n",
    "There are infinitely many possible values for the weights and bias. We can't try them all, and there's no obvious formula that tells us the best ones.  \n",
    "So instead of guessing randomly or exhaustively searching, we need an **intelligent way to search** for a good solution.\n",
    "\n",
    "That strategy is called **gradient descent**.\n",
    "\n",
    "### The Basic Idea\n",
    "\n",
    "Imagine you're standing somewhere on a hilly landscape, and your goal is to get to the bottom of the valley — where the **loss** is lowest.\n",
    "\n",
    "You can’t see the whole terrain, but you can feel which direction the ground slopes. So you take a step downhill. Then another. Then another.  \n",
    "Eventually, you get closer and closer to the lowest point.\n",
    "\n",
    "This is exactly what **gradient descent** does — but instead of navigating a landscape, it’s searching through possible weights for your model.  \n",
    "It moves step-by-step in the direction that **reduces the loss**.\n",
    "\n",
    "> **Gradient descent is an optimization algorithm that tweaks the model’s weights to reduce the loss.**\n",
    "\n",
    "### How It Works\n",
    "\n",
    "At each training step:\n",
    "\n",
    "1. The model makes a prediction using the current weights and bias\n",
    "2. The prediction is compared to the true answer using the **loss function**\n",
    "3. The **gradient** of the loss is computed — how much the loss would change if we nudged each weight a little\n",
    "4. The weights are updated by taking a small step in the direction that reduces the loss\n",
    "\n",
    "This step is done for **each weight** and the bias.\n",
    "\n",
    "#### Update Rule\n",
    "\n",
    "The standard update formula looks like this:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80540f15",
   "metadata": {},
   "source": [
    "```wᵢ ← wᵢ - η * ∂L/∂wᵢ\n",
    "\n",
    "\n",
    "\n",
    "- `wᵢ` is the current weight  \n",
    "- `∂L/∂wᵢ` is the gradient of the loss with respect to that weight  \n",
    "- `η` (eta) is the **learning rate** — a small constant that controls the step size  \n",
    "- The new weight is the old weight **minus** a small step in the direction of the gradient\n",
    "\n",
    "We do the same for the bias `b`:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06535fc",
   "metadata": {},
   "source": [
    "```None\n",
    "b ← b - η * ∂L/∂b\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b4463",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "> The minus sign is important — it tells us to move **downhill**, not uphill.\n",
    "\n",
    "### The Learning Rate\n",
    "\n",
    "The learning rate `η` is a small number like `0.01` or `0.001`. It controls **how big each step is**.\n",
    "\n",
    "- If it's too small, training will be slow  \n",
    "- If it's too big, training can overshoot the minimum and never settle\n",
    "\n",
    "Finding a good learning rate is part of tuning the training process.\n",
    "\n",
    "### A Simple Example\n",
    "\n",
    "Let’s say we have a model with just one weight:\n",
    "\n",
    "- The current weight is `w = 2.0`  \n",
    "- The loss function tells us that increasing `w` increases the loss  \n",
    "- The gradient is `∂L/∂w = 4.0`  \n",
    "- We’re using a learning rate of `η = 0.1`\n",
    "\n",
    "We update the weight:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59274676",
   "metadata": {},
   "source": [
    "```w ← 2.0 - 0.1 * 4.0 = 1.6\n",
    "\n",
    "\n",
    "\n",
    "We’ve taken a small step **toward lower loss**.\n",
    "\n",
    "On the next step, we repeat the process — compute a new prediction, get the new loss and gradient, and update again.  \n",
    "This process continues until the loss stops improving (or we reach a set number of steps).\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "The magic of gradient descent is that it **uses the slope of the loss function** to guide the search.\n",
    "\n",
    "- If the slope is steep, the weight update is bigger  \n",
    "- If we’re close to the minimum, the slope flattens, and the updates become smaller  \n",
    "- Over time, the model settles into a **minimum-loss** solution\n",
    "\n",
    "> **Key idea:**  \n",
    "> Gradient descent turns learning into a process of repeated small improvements,  \n",
    "> guided by math, not guesswork.\n",
    "\n",
    "### Will It Always Find the Best Solution?\n",
    "\n",
    "Not necessarily.\n",
    "\n",
    "Gradient descent is a powerful tool, but it doesn’t **guarantee** that we’ll find the best possible set of weights — the true global minimum of the loss function.\n",
    "\n",
    "Why not?\n",
    "\n",
    "- The **loss landscape** can be complicated, especially for deep neural networks\n",
    "- It might have **multiple valleys** (local minima), **flat regions**, or **noisy slopes**\n",
    "- Gradient descent can get stuck in a **local minimum**, or just settle somewhere “good enough”\n",
    "\n",
    "In practice, this usually isn’t a big problem. For many real-world tasks, getting to a **low-loss** solution (even if it's not the lowest possible) is good enough — and gradient descent gets us there surprisingly well.\n",
    "\n",
    "> So while gradient descent doesn’t always find *the best*, it often finds something **very useful** — and it does it fast.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "When we train a neural network using gradient descent, we need to compute how wrong the model is — the **loss** — and then use that to update the weights.\n",
    "\n",
    "But here’s an important question:\n",
    "\n",
    "> **How much data should we use to compute that update?**\n",
    "\n",
    "There are actually a few different strategies, and they each have tradeoffs.\n",
    "\n",
    "#### 1. Full-Batch Gradient Descent\n",
    "\n",
    "This is the most straightforward idea:\n",
    "\n",
    "- Use the **entire training dataset** to compute the average loss  \n",
    "- Then update the weights once, using the overall gradient\n",
    "\n",
    "\n",
    "#### 2. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "At the opposite extreme, we can update the model using **just one example at a time**:\n",
    "\n",
    "- Pick one training example  \n",
    "- Compute the loss and gradient  \n",
    "- Update the weights  \n",
    "- Repeat\n",
    "\n",
    "This is called **stochastic gradient descent** — \"stochastic\" meaning random. It updates the weights using **noisy**, fast updates.\n",
    "\n",
    "#### 3. Mini-Batch Gradient Descent (the most common)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates the model using one training example at a time, which makes each update very fast. This can be a big advantage when working with large datasets. Because it's based on just a single example, SGD introduces some randomness (\"noise\") into the updates — which can help the model escape local minimums but also makes training less stable. Full-batch gradient descent, on the other hand, uses the entire dataset to compute each update. This gives a very accurate gradient and makes learning more stable and predictable — but each update is much slower, and it requires a lot more memory. In practice, most modern training uses a compromise: **mini-batch gradient descent**, which combines the speed of SGD with the stability of full-batch.\n",
    "\n",
    "> **Mini-batch gradient descent** uses a small number of examples at a time — usually 32, 64, or 128.\n",
    "\n",
    "Each step works like this:\n",
    "\n",
    "- Pick a small **batch** of examples (e.g. 64 messages)\n",
    "- Compute the average loss and gradient for just that batch\n",
    "- Update the weights\n",
    "- Move to the next batch\n",
    "\n",
    "### Epochs\n",
    "\n",
    "When we say we’ve done one **epoch** of training, it means we’ve gone through the **entire training dataset once**. Training usually takes **multiple epochs**, so the model gets many chances to improve. Over time, the weights are refined to make better and better predictions.\n",
    "\n",
    "\n",
    "## Deriving the Gradient of MSE Loss for Linear Regression\n",
    "\n",
    "Let’s derive the gradient we need for training a linear regression model using gradient descent.\n",
    "\n",
    "### Step 1: The Model\n",
    "\n",
    "We start with the linear regression equation:\n",
    "\n",
    "**ŷ = w · x + b**\n",
    "\n",
    "Where:  \n",
    "- `x` is the input vector (a 1D tensor of features)  \n",
    "- `w` is the weight vector (same size as `x`)  \n",
    "- `b` is the bias (a scalar)  \n",
    "- `ŷ` is the predicted output (a scalar)\n",
    "\n",
    "### Step 2: The Loss Function\n",
    "\n",
    "We’ll use **Mean Squared Error (MSE)** as our loss function.  \n",
    "For a single training example:\n",
    "\n",
    "**L = (ŷ - y)²**\n",
    "\n",
    "Where:  \n",
    "- `ŷ = w · x + b` is the predicted value  \n",
    "- `y` is the true value from the training data\n",
    "\n",
    "We want to compute the gradients of `L` with respect to `w` and `b`.\n",
    "\n",
    "### Step 3: Gradient with Respect to Weights (w)\n",
    "\n",
    "We use the chain rule:\n",
    "\n",
    "**dL/dw = dL/dŷ · dŷ/dw**\n",
    "\n",
    "First:\n",
    "\n",
    "**dL/dŷ = 2(ŷ - y)**\n",
    "\n",
    "Then:\n",
    "\n",
    "**dŷ/dw = x**\n",
    "\n",
    "Putting it together:\n",
    "\n",
    "**dL/dw = 2(ŷ - y) · x**\n",
    "\n",
    "### Step 4: Gradient with Respect to Bias (b)\n",
    "\n",
    "Again, using the chain rule:\n",
    "\n",
    "**dL/db = dL/dŷ · dŷ/db**\n",
    "\n",
    "We already have:\n",
    "\n",
    "**dL/dŷ = 2(ŷ - y)**\n",
    "\n",
    "And:\n",
    "\n",
    "**dŷ/db = 1**\n",
    "\n",
    "So:\n",
    "\n",
    "**dL/db = 2(ŷ - y)**\n",
    "\n",
    "### Final Gradient Equations\n",
    "\n",
    "For one training example:\n",
    "\n",
    "- **Gradient of the weights:**  \n",
    "  **∇w L = 2(ŷ - y) · x**\n",
    "\n",
    "- **Gradient of the bias:**  \n",
    "  **∇b L = 2(ŷ - y)**\n",
    "\n",
    "These gradients tell us how to update the weights and bias in the direction that reduces the loss.\n",
    "\n",
    "### Note on Averaging (for Mini-Batches)\n",
    "\n",
    "If you're working with a **batch of examples**, you would average the gradients over the batch:\n",
    "\n",
    "- **∇w L = average over batch of [2(ŷ - y) · x]**  \n",
    "- **∇b L = average over batch of [2(ŷ - y)]**\n",
    "\n",
    "This ensures that the update reflects the overall trend across the examples, not just a single case.\n",
    "\n",
    "## Predicting Ice Cream Sales\n",
    "\n",
    "Imagine you run an **ice cream shop**, and you’ve recorded your **daily ice cream sales** alongside the **temperature** on that day.\n",
    "\n",
    "We want to build a model that can predict **ice cream sales (`y`)** based on the **temperature (`x`)**.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We'll use a simple **synthetic dataset** that simulates our real-world scenario. The dataset is generated using this rough formula:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04725c77",
   "metadata": {},
   "source": [
    "```None\n",
    "y ≈ 126 + 13x + noise\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5a64b",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "- `x` is the temperature (in degrees Celsius), randomly sampled between 0 and 30  \n",
    "- `y` is the number of ice cream cones sold, with some random noise added to simulate real-world variability  \n",
    "- The noise makes the relationship more realistic — it's not perfectly linear, just mostly linear\n",
    "\n",
    "This synthetic data allows us to:\n",
    "- Focus on the **core ideas** of linear regression\n",
    "- Keep the math and code simple\n",
    "- Easily **visualize** the data and the model, since it has only one feature\n",
    "\n",
    "We’ve wrapped the dataset in a small Python class called `IceCreamDataSet` that lets us generate and plot the data easily.\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716998f9",
   "metadata": {},
   "source": [
    "```import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(13)\n",
    "\n",
    "class IceCreamDataSet:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.n = 100\n",
    "    self.x = [random.uniform(0,30) for _ in range(self.n)]\n",
    "    self.y = [round(max(0,126 + 13*xi + random.normalvariate(0,26))) for xi in self.x]\n",
    "\n",
    "  def plot_data(self):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(self.x,self.y)\n",
    "    plt.xlabel(\"Temperature (°C)\")\n",
    "    plt.ylabel(\"Ice Cream Sales\")\n",
    "    plt.title(\"Ice Cream Sales vs. Temperature\")\n",
    "    plt.show()\n",
    "\n",
    "  def plot_model(self,model):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(self.x,self.y)\n",
    "    line_x = [min(self.x), max(self.x)]\n",
    "    line_y = [model.forward(xi) for xi in line_x]\n",
    "    plt.plot(line_x,line_y,color='red')\n",
    "    plt.xlabel(\"Temperature (°C)\")\n",
    "    plt.ylabel(\"Ice Cream Sales\")\n",
    "    plt.title(\"Model Prediction vs. Data\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "Here, we create an instance of the synthetic dataset, simulating 100 days of ice cream sales based on daily temperature.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IceCreamDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f095e",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Let's show a scatterplot of the data, where each point represents one day's temperature and the corresponding number of ice cream cones sold.\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a8ab6",
   "metadata": {},
   "source": [
    "```dataset.plot_data()\n",
    "\n",
    "\n",
    "\n",
    "### Building a Simple Linear Regression Model\n",
    "\n",
    "Now that we have a dataset, we need a model to learn from it.\n",
    "\n",
    "We’ll create a simple **linear regression model** from scratch. The model will predict ice cream sales using this formula:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e93dd",
   "metadata": {},
   "source": [
    "```None\n",
    "ŷ = w * x + b\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d0396",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x` is the input (temperature)\n",
    "- `ŷ` is the predicted output (sales)\n",
    "- `w` is the **weight** (slope), and `b` is the **bias** (intercept) — both are parameters the model will learn\n",
    "\n",
    "Here’s a class that defines the model and its two core operations:\n",
    "\n",
    "- `forward`: Makes a prediction\n",
    "- `backward`: Computes gradients of the loss with respect to `w` and `b`, using Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9928b",
   "metadata": {},
   "source": [
    "```class LinearModel:\n",
    "    \"\"\"\n",
    "    A simple linear regression model:\n",
    "    ŷ = w * x + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize weights and bias to zero\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Predict the output ŷ given input x.\n",
    "        \"\"\"\n",
    "        return self.w * x + self.b\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute gradients of the loss with respect to w and b\n",
    "        using Mean Squared Error (MSE) for a single example.\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(x)\n",
    "        error = y_pred - y\n",
    "        dw = 2 * error * x   # gradient w.r.t. w\n",
    "        db = 2 * error       # gradient w.r.t. b\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "\n",
    "Let's use the class to create a model. \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc567679",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Let's take a look at the initial weights:\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97974873",
   "metadata": {},
   "source": [
    "```dataset.plot_model(model)\n",
    "\n",
    "\n",
    "\n",
    "#### Why Is It Called \"Backward\"?\n",
    "\n",
    "The function that computes the gradient is called `backward` because it’s about working **backward from the error** to figure out how to adjust the model.\n",
    "\n",
    "In the **forward** step, the model takes an input (like temperature), runs it through the equation (`ŷ = w * x + b`), and produces a prediction.  \n",
    "\n",
    "Then we compare that prediction to the true answer and calculate how wrong it was — the **loss**.\n",
    "\n",
    "In the **backward** step, we take that loss and ask:  \n",
    "> *How should we change the weights and bias to reduce this error next time?*\n",
    "\n",
    "To answer that, we work backward through the equation, computing how much each part (each parameter) contributed to the mistake. This is the start of a process called **backpropagation** — short for *backward propagation of errors*.\n",
    "\n",
    "In more complex networks, this process involves the **chain rule** from calculus, which allows us to pass gradients backward through layers of functions.  \n",
    "We’ll see that later — but even now, you’re seeing the core idea:  \n",
    "> *To learn, a model must look at the error and trace it back to the parts that caused it.*\n",
    "\n",
    "### Training the Model (One Example at a Time)\n",
    "\n",
    "Now that we have a model and a dataset, let’s train the model using **gradient descent**.\n",
    "\n",
    "We'll use a very simple form of training called **stochastic gradient descent**. That means we’ll update the model **one example at a time** — using just a single `(x, y)` pair per update.\n",
    "\n",
    "For each training example:\n",
    "\n",
    "1. Make a prediction using the model  \n",
    "2. Compare it to the actual value to get the error  \n",
    "3. Use the `backward()` method to compute the gradients  \n",
    "4. Adjust the model’s parameters (`w` and `b`) to reduce the error\n",
    "\n",
    "To visualize the training process, we’ll also plot the model’s current line after each epoch, and pause briefly so we can see it evolve.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set the learning rate — controls how big each update is\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Train for 100 passes (epochs) over the dataset\n",
    "for epoch in range(100):\n",
    "    for i in range(dataset.n):\n",
    "        # Get one example (temperature and sales)\n",
    "        x = dataset.x[i]\n",
    "        y = dataset.y[i]\n",
    "\n",
    "        # Forward pass: make a prediction\n",
    "        y_pred = model.forward(x)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        dw, db = model.backward(x, y)\n",
    "\n",
    "        # Update weights and bias using gradient descent\n",
    "        model.w -= learning_rate * dw\n",
    "        model.b -= learning_rate * db\n",
    "\n",
    "    # Clear previous plot/output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Print current progress\n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"Learned weight (w):\", model.w)\n",
    "    print(\"Learned bias (b):\", model.b)\n",
    "\n",
    "    # Plot the current model line\n",
    "    dataset.plot_model(model)\n",
    "\n",
    "    # Pause so we can see the update before moving on\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ab565",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "## Multiclass Logistic Regression\n",
    "\n",
    "So far, we’ve looked at **linear regression**, where the model predicts a **single number**. Now let’s extend this idea to a classification task, where the goal is to predict **which category** something belongs to.\n",
    "\n",
    "### From Regression to Classification\n",
    "\n",
    "In classification, instead of predicting a single continuous value, we want to predict one of several **classes**. \n",
    "\n",
    "For example, we might want to classify a customer email as:\n",
    "\n",
    "- `0` = Billing  \n",
    "- `1` = Sales  \n",
    "- `2` = Tech Support\n",
    "\n",
    "### Outputs as a Vector\n",
    "\n",
    "Instead of outputting a single number `ŷ`, the model now outputs a **vector of values** — one for each class.\n",
    "\n",
    "For example, the model might output a vector like this:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67638a42",
   "metadata": {},
   "source": [
    "```ŷ = [0.1, 0.7, 0.2]\n",
    "\n",
    "\n",
    "\n",
    "We can interpret this as:\n",
    "\n",
    "- 10% chance it's Billing  \n",
    "- 70% chance it's Sales  \n",
    "- 20% chance it's Tech Support  \n",
    "\n",
    "And we would classify this example as **Sales**, since it has the highest score.\n",
    "\n",
    "### Model Equation\n",
    "\n",
    "Our model equation is now:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df939a6",
   "metadata": {},
   "source": [
    "```None\n",
    "ŷ = softmax(w · x + b)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36db89d",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Let’s break it down:\n",
    "\n",
    "- `x` is the **input vector** (with `n` features)\n",
    "- `w` is the **weight matrix**, with shape `k × n`  \n",
    "  (one row of weights for each of the `k` classes)\n",
    "- `b` is the **bias vector**, with one bias per class\n",
    "- The result `w · x + b` is a **vector of raw scores** (called **logits**)\n",
    "- We apply a function called **softmax** to turn those scores into **probabilities**\n",
    "\n",
    "### Softmax: From Scores to Probabilities\n",
    "\n",
    "Softmax is a type of **activation function** — a function applied at the output of a model to shape or interpret the result in some useful way.\n",
    "\n",
    "The softmax function takes in a vector of raw scores and returns a vector of probabilities that sum to 1. It’s defined as:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555968d",
   "metadata": {},
   "source": [
    "```softmax(zᵢ) = exp(zᵢ) / Σⱼ exp(zⱼ)\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- `zᵢ` is the raw score for class `i` (the `i`th element of the logits vector)\n",
    "- `exp(zᵢ)` makes all the scores positive and amplifies larger ones\n",
    "- The denominator ensures the outputs sum to 1\n",
    "\n",
    "> This lets us interpret the outputs as a **probability distribution** over the classes.\n",
    "\n",
    "### Why Is `w` a Matrix?\n",
    "\n",
    "In linear regression, we had:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d093f",
   "metadata": {},
   "source": [
    "```None\n",
    "ŷ = w · x + b\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b1d19",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Here, `w` was a vector (1D), and `ŷ` was a single number.\n",
    "\n",
    "But in classification, we want **one output per class**, so we need **a different set of weights for each class**. That means `w` becomes a **matrix**:\n",
    "\n",
    "- Each **row** of `w` corresponds to one class\n",
    "- The dot product between that row and the input `x` gives a **score** for that class\n",
    "\n",
    "In effect, it’s like running **multiple linear regressions in parallel** — one for each class — and then using softmax to pick the most likely one.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Multiclass logistic regression is a natural extension of linear regression:\n",
    "\n",
    "- We go from a **single output** to a **vector of outputs**\n",
    "- We go from a **weight vector** to a **weight matrix**\n",
    "- We apply a **nonlinear activation function** (softmax) to interpret the outputs as probabilities\n",
    "\n",
    "## Deriving the Gradient: Multiclass Logistic Regression\n",
    "\n",
    "We’ll now derive the gradient for a **softmax classifier** trained with **cross-entropy loss**.\n",
    "\n",
    "### Model Output\n",
    "\n",
    "The model computes:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f9e03",
   "metadata": {},
   "source": [
    "```z = w · x + b         # raw scores (logits)\n",
    "ŷ = softmax(z)        # predicted probabilities\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- `x` is the input vector (length `n`)\n",
    "- `w` is a weight matrix with shape `k × n` (k = number of classes)\n",
    "- `b` is a bias vector (length `k`)\n",
    "- `ŷ` is a vector of class probabilities (length `k`)\n",
    "\n",
    "The softmax function for class `i` is:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64872d1",
   "metadata": {},
   "source": [
    "```None\n",
    "ŷᵢ = exp(zᵢ) / sum_j exp(zⱼ)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf1037",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "### Loss Function: Cross-Entropy\n",
    "\n",
    "If the true class label is `y` (an integer from `0` to `k-1`), the loss is:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447f32c",
   "metadata": {},
   "source": [
    "```L = -log(ŷ_y)        # negative log of the predicted probability for the true class\n",
    "\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "We want to compute the gradients:\n",
    "\n",
    "- ∂L/∂wᵢⱼ — how the loss changes with respect to each weight\n",
    "- ∂L/∂bᵢ  — how the loss changes with respect to each bias\n",
    "\n",
    "### Gradient of the Loss\n",
    "\n",
    "We apply the chain rule. The derivative of the loss with respect to the logit `zᵢ` is:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec600e",
   "metadata": {},
   "source": [
    "```None\n",
    "∂L/∂zᵢ = ŷᵢ - 1   if i == y\n",
    "∂L/∂zᵢ = ŷᵢ       otherwise\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa8e2a",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "This tells us that:\n",
    "\n",
    "- For the **correct class**, we subtract 1 from the predicted probability\n",
    "- For all other classes, the gradient is just the predicted probability\n",
    "\n",
    "### Final Gradients\n",
    "\n",
    "Now that we have ∂L/∂zᵢ, we compute the gradients with respect to the weights and biases:\n",
    "\n",
    "\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18f45f",
   "metadata": {},
   "source": [
    "```∂L/∂wᵢⱼ = (ŷᵢ - 1[y = i]) * xⱼ\n",
    "∂L/∂bᵢ  = (ŷᵢ - 1[y = i])\n",
    "\n",
    "\n",
    "\n",
    "Where `1[y = i]` is 1 if `i` is the correct class, otherwise 0.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The **error** for each class is:  \n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d08e3",
   "metadata": {},
   "source": [
    "```None\n",
    "  errorᵢ = ŷᵢ - 1[y = i]\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c16f75",
   "metadata": {},
   "source": [
    "```\n",
    "- We multiply that error by each input feature to get the gradient:\n",
    "  \n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273a3d9",
   "metadata": {},
   "source": [
    "```  dwᵢⱼ = errorᵢ * xⱼ\n",
    "  dbᵢ  = errorᵢ\n",
    "  \n",
    "\n",
    "\n",
    "This gives us the gradients we need to perform gradient descent on a multiclass softmax classifier.\n",
    "\n",
    "## SMS Spam Collection\n",
    "\n",
    "To see how logistic regressions work, let's try building one. We'll use the same dataset we used last week. \n",
    "\n",
    "### Dataset, Tokenizer, and Vectorizer\n",
    "\n",
    "First, we'll download the data and split it into test and train datasets.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141569cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget https://wd13ca.github.io/BAN200-Summer-2025/SMSSpamCollection.txt\n",
    "\n",
    "# Load it into a pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"SMSSpamCollection.txt\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset: 80% training, 20% testing\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show the number of messages in each set\n",
    "print(\"Training messages:\", len(train_df))\n",
    "print(\"Test messages:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f93ad1",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Next, we'll recreate our tokenizer.\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95076cfc",
   "metadata": {},
   "source": [
    "```import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', lowercase_text)\n",
    "    return [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "\n",
    "We'll also need a vectorizer. Let's use the one from Week 3. \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af26812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "N = len(train_df) # total number of documents\n",
    "\n",
    "doc_freq = {} # document frequency for each word\n",
    "for _, row in train_df.iterrows():\n",
    "  tokens = tokenize(row[\"message\"])\n",
    "  unique_tokens = set(tokens) # only count once per document\n",
    "  for token in unique_tokens:\n",
    "      doc_freq[token] = doc_freq.get(token, 0) + 1\n",
    "\n",
    "idf_dict = {}\n",
    "for token, df in doc_freq.items():\n",
    "    idf_dict[token] = log(N / df)\n",
    "\n",
    "def vectorize_tf_idf(tokens):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and an IDF dictionary,\n",
    "    returns a dictionary representing the TF-IDF vector.\n",
    "    \"\"\"\n",
    "    tf = {}\n",
    "    for token in tokens:\n",
    "        if token in tf:\n",
    "            tf[token] += 1\n",
    "        else:\n",
    "            tf[token] = 1\n",
    "\n",
    "    tf_idf = {}\n",
    "    for token, freq in tf.items():\n",
    "        if token in idf_dict:\n",
    "            tf_idf[token] = freq * idf_dict[token]\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5589401",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "### Defining a Model Class\n",
    "\n",
    "To build a multiclass classification model from scratch, we’ll use **multiclass logistic regression** with:\n",
    "\n",
    "- A **softmax activation function** to produce probabilities\n",
    "- A **cross-entropy loss** to measure how wrong the predictions are\n",
    "- **Gradient descent** to update the weights and bias\n",
    "\n",
    "We’ll define a Python class called `BoWLogisticClassifier` to encapsulate everything: prediction (`forward`), gradient computation (`backward`), and model parameters (`w` and `b`).\n",
    "\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0797a1",
   "metadata": {},
   "source": [
    "```import math\n",
    "\n",
    "class BoWLogisticClassifier:\n",
    "    \"\"\"\n",
    "    A sparse multiclass logistic regression model with softmax activation.\n",
    "    Input vectors x are sparse dictionaries: {feature: value}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, classes):\n",
    "        \"\"\"\n",
    "        Initialize model with zero weights and zero biases.\n",
    "        - vocab: a set of input feature names (e.g., words)\n",
    "        - classes: a set of class labels (e.g., 'spam', 'ham')\n",
    "        \"\"\"\n",
    "        self.vocab = sorted(vocab)\n",
    "        self.classes = sorted(classes)\n",
    "        self.input_dim = len(self.vocab)\n",
    "        self.output_dim = len(self.classes)\n",
    "\n",
    "        # Map vocab and classes to index\n",
    "        self.vocab_index = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.class_index = {label: i for i, label in enumerate(self.classes)}\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.w = [[0.0 for _ in range(self.input_dim)] for _ in range(self.output_dim)]\n",
    "        self.b = [0.0 for _ in range(self.output_dim)]\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Apply softmax to logits for numerical stability.\n",
    "        \"\"\"\n",
    "        max_logit = max(logits)\n",
    "        exp_scores = [math.exp(z - max_logit) for z in logits]\n",
    "        sum_exp = sum(exp_scores)\n",
    "        return [s / sum_exp for s in exp_scores]\n",
    "\n",
    "    def forward(self, x_sparse):\n",
    "        \"\"\"\n",
    "        Compute prediction for sparse input x_sparse: dict {feature: value}.\n",
    "        Returns probability distribution over classes.\n",
    "        \"\"\"\n",
    "        logits = []\n",
    "        for k in range(self.output_dim):\n",
    "            score = self.b[k]\n",
    "            for word, value in x_sparse.items():\n",
    "                if word in self.vocab_index:\n",
    "                    j = self.vocab_index[word]\n",
    "                    score += self.w[k][j] * value\n",
    "            logits.append(score)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def backward(self, x_sparse, y_true_label):\n",
    "        \"\"\"\n",
    "        Compute gradients w.r.t weights and biases using cross-entropy loss.\n",
    "        - x_sparse: input features as dict {feature: value}\n",
    "        - y_true_label: true class label (e.g., 'spam')\n",
    "\n",
    "        Returns:\n",
    "            dw: sparse gradient of weights (dict of dicts)\n",
    "            db: dense gradient of biases (list)\n",
    "        \"\"\"\n",
    "        y_true = self.class_index[y_true_label]\n",
    "        y_pred = self.forward(x_sparse)\n",
    "\n",
    "        # Gradients\n",
    "        dw = {k: {} for k in range(self.output_dim)}\n",
    "        db = [0.0 for _ in range(self.output_dim)]\n",
    "\n",
    "        for k in range(self.output_dim):\n",
    "            error = y_pred[k] - (1 if k == y_true else 0)\n",
    "            for word, value in x_sparse.items():\n",
    "                if word in self.vocab_index:\n",
    "                    j = self.vocab_index[word]\n",
    "                    dw[k][word] = error * value\n",
    "            db[k] = error\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "\n",
    "### Training the Classifier\n",
    "\n",
    "Now let’s train our logistic regression model using gradient descent.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d224a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Build vocabulary and class labels from training set\n",
    "all_tokens = [token for message in train_df[\"message\"] for token in tokenize(message)]\n",
    "vocab = set(all_tokens)\n",
    "classes = set(train_df[\"label\"])\n",
    "\n",
    "# Initialize model\n",
    "model = BoWLogisticClassifier(vocab=vocab, classes=classes)\n",
    "\n",
    "# Prepare training data as (x_sparse, y_label) pairs\n",
    "train_data = []\n",
    "for _, row in train_df.iterrows():\n",
    "    tokens = tokenize(row[\"message\"])\n",
    "    x_sparse = vectorize_tf_idf(tokens)\n",
    "    y_label = row[\"label\"]\n",
    "    train_data.append((x_sparse, y_label))\n",
    "\n",
    "# Training loop\n",
    "def train(model, data, lr=0.1, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(data)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x_sparse, y_label in data:\n",
    "            y_true_idx = model.class_index[y_label]\n",
    "            y_pred = model.forward(x_sparse)\n",
    "\n",
    "            # Cross-entropy loss\n",
    "            loss = -math.log(y_pred[y_true_idx] + 1e-12)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Compute gradients\n",
    "            dw, db = model.backward(x_sparse, y_label)\n",
    "\n",
    "            # Update weights and biases\n",
    "            for k in range(model.output_dim):\n",
    "                for word, grad in dw[k].items():\n",
    "                    j = model.vocab_index[word]\n",
    "                    model.w[k][j] -= lr * grad\n",
    "                model.b[k] -= lr * db[k]\n",
    "\n",
    "        avg_loss = total_loss / len(data)\n",
    "        print(f\"Epoch {epoch + 1}: Average Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, train_data, lr=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee9e36",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "### Predict Function\n",
    "\n",
    "Let's create a `predict()` function that takes an unlabeled message and returns a predicted label:\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165dd19",
   "metadata": {},
   "source": [
    "```def predict(message, model, tokenizer, vectorizer):\n",
    "    \"\"\"\n",
    "    Predict the class label for a raw input message (string).\n",
    "    - message: the input message (e.g., \"Free entry now!!!\")\n",
    "    - tokenizer: a function that tokenizes the message\n",
    "    - vectorizer: a function that turns tokens into sparse vector\n",
    "\n",
    "    Returns:\n",
    "        predicted_label: the class label with highest probability\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(message)\n",
    "    x_sparse = vectorizer(tokens)\n",
    "    probs = model.forward(x_sparse)\n",
    "    predicted_idx = probs.index(max(probs))\n",
    "    return model.classes[predicted_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Evaluate Model Accuracy on the Test Set\n",
    "\n",
    "Now that we have a working `predict()` function, we’ll apply it to every message in the test set.\n",
    "\n",
    "Then we’ll compare the predicted labels to the actual labels and calculate the model’s accuracy.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all messages in the test set\n",
    "predictions = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    message = row[\"message\"]\n",
    "    prediction = predict(message,model,tokenize,vectorize_tf_idf)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Actual labels\n",
    "actual = test_df[\"label\"].tolist()\n",
    "\n",
    "# Compute accuracy\n",
    "correct = sum([pred == truth for pred, truth in zip(predictions, actual)])\n",
    "accuracy = correct / len(test_df)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3463f",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Now let's take a look at the confusion matrix:\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de6812",
   "metadata": {},
   "source": [
    "```from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(actual, predictions, labels=[\"spam\", \"ham\"])\n",
    "\n",
    "# Display as a readable table\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"               Predicted\")\n",
    "print(f\"             | spam | ham \")\n",
    "print(f\"Actual spam  |  {cm[0][0]:4} | {cm[0][1]:4}\")\n",
    "print(f\"Actual ham   |  {cm[1][0]:4} | {cm[1][1]:4}\")\n",
    "\n",
    "\n",
    "\n",
    "### Precision, Recall, and F1-Score\n",
    "\n",
    "Here are the precision, recall, and F1-scores:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(actual, predictions, target_names=[\"spam\", \"ham\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee57892",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Let’s look at some misclassified messages — where the model's prediction didn't match the true label.\n",
    "\n",
    "This helps us understand:\n",
    "\n",
    "- Where the model is confused\n",
    "- Whether certain types of spam are being missed\n",
    "- If it’s too aggressive (labeling ham as spam)\n",
    "\n",
    "\n",
    "python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce32fe",
   "metadata": {},
   "source": [
    "```# Show the first 10 misclassified messages\n",
    "for i in range(len(test_df)):\n",
    "    if predictions[i] != actual[i]:\n",
    "        print(f\"\\n--- Misclassified Message ---\")\n",
    "        print(f\"Actual:    {actual[i]}\")\n",
    "        print(f\"Predicted: {predictions[i]}\")\n",
    "        print(f\"Message:   {test_df.iloc[i]['message']}\")\n",
    "\n",
    "\n",
    "## Logistic Regression vs. Naive Bayes: Why Logistic Regression Sometimes Performs Worse\n",
    "\n",
    "You may have noticed that our **logistic regression model** doesn't perform quite as well as the **Naive Bayes model** on the SMS Spam Collection dataset. This might seem surprising at first — after all, logistic regression is more flexible and learned its weights directly from the data. So what’s going on?\n",
    "\n",
    "### Why Logistic Regression Might Underperform\n",
    "\n",
    "Here are a few possible reasons:\n",
    "\n",
    "1. **Naive Bayes is surprisingly strong for text classification**  \n",
    "   Even though it makes strong independence assumptions (i.e. that words are independent given the class), those assumptions actually hold *reasonably well* in many text datasets. This makes Naive Bayes hard to beat, especially on short documents like SMS messages.\n",
    "\n",
    "2. **Logistic regression is more sensitive to sparse features**  \n",
    "   Logistic regression must learn one weight per feature, and when the vocabulary is large and many features appear rarely (as in text), it can be harder for the model to learn stable estimates — especially with limited training data.\n",
    "\n",
    "3. **No regularization in our implementation**  \n",
    "   Our logistic regression model is very basic: it doesn’t use techniques like **L2 regularization**, which can prevent overfitting and improve generalization — especially with high-dimensional sparse inputs like TF-IDF vectors.\n",
    "\n",
    "4. **Naive Bayes builds in strong priors**  \n",
    "   Naive Bayes directly incorporates class priors (how common spam vs. ham messages are), and it’s especially effective when those priors are unbalanced. Logistic regression can learn this too, but it requires more data and care.\n",
    "\n",
    "### Why Learn Logistic Regression Anyway?\n",
    "\n",
    "Even though Naive Bayes outperforms logistic regression *here*, logistic regression is still **worth learning — and essential going forward**:\n",
    "\n",
    "- It introduces the idea of **learning weights from data**, not from counts or rules  \n",
    "- It teaches the concept of **gradient descent**, which we’ll use to train more complex models  \n",
    "- It prepares us for **deep learning**, where logistic regression is effectively the **final layer** of most neural networks used for classification  \n",
    "- It scales better to complex, high-dimensional data once regularization and optimization improvements are added\n",
    "\n",
    "> **Key takeaway:**  \n",
    "> Logistic regression might not win on this dataset — but it’s a critical stepping stone toward more powerful and general models, like deep neural networks and transformers.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This week, we explored the building blocks of modern neural networks and machine learning through hands-on implementations and intuitive explanations:\n",
    "\n",
    "- We introduced **artificial neural networks** as powerful pattern-matching systems\n",
    "- We learned how **tensors** serve as the basic data structure for neural models\n",
    "- We re-framed **linear regression** as a single-layer neural network\n",
    "- We studied **loss functions** — especially **mean squared error (MSE)** — to measure how wrong a model is\n",
    "- We implemented **gradient descent**, the algorithm that adjusts model weights to reduce error\n",
    "- We trained a model to predict **ice cream sales from temperature**, step by step\n",
    "- We extended linear models to **multiclass classification** with **softmax** and **cross-entropy loss**\n",
    "- We built a complete **logistic regression classifier** for spam detection using sparse TF-IDF vectors\n",
    "\n",
    "Even though our logistic regression model didn’t outperform Naive Bayes on this dataset, it marks an important transition: we are now building models that **learn directly from data**, laying the foundation for deep learning and neural architectures we’ll explore in future weeks.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Try the following exercises to reinforce this week’s concepts:\n",
    "\n",
    "1. **Manual Prediction**  \n",
    "   Using the linear regression example from class, compute the prediction for a new house with features:  \n",
    "   `x₁ = 1500`, `x₂ = 4`, `x₃ = 0`  \n",
    "   Use the same weights as in the house price example.\n",
    "\n",
    "2. **Derive the Gradient**  \n",
    "   Walk through the derivation of the MSE gradient for a single data point using the chain rule.  \n",
    "   Explain what each part means in plain English.\n",
    "\n",
    "3. **Experiment with Learning Rate**  \n",
    "   Modify the ice cream training code to try different learning rates. What happens if you set it to `0.0001` or `0.1`?\n",
    "\n",
    "4. **Inspect the Weights**  \n",
    "   Print the 10 largest and smallest weights from your logistic regression model.  \n",
    "   What kinds of words are getting high/low weights?\n",
    "\n",
    "5. **Backpropagation Reflection**  \n",
    "   In your own words, explain why the `.backward()` function is called \"backward\".  \n",
    "   What’s being traced backward, and why is that important?\n",
    "\n",
    "6. **Try a New Dataset**  \n",
    "   Replace the SMS spam dataset with a small custom dataset (e.g., short labeled tweets or comments) and try retraining the logistic regression model.\n",
    "\n",
    "\n",
    "## Group Project Reminder\n",
    "\n",
    "Each group must meet with me to discuss their Project Proposal. Meetings will take place during the Week 7 class. Not all members of the group need to attend. If no one from your group is able to attend during class time, please email me by June 8 to make an accommodation.\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
