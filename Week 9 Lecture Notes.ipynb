{"cells":[{"cell_type":"markdown","metadata":{"id":"FrprP7b32am5"},"source":["# Week 9 Lecture Notes\n","\n","## Review: Linear and Logistic Regression\n","\n","Before we move on to neural networks, let’s take a step back and review the models we've built so far — **linear regression** and **logistic regression**.\n","\n","These two models illustrate a powerful general recipe for building machine learning systems:\n","\n","### The Four Ingredients of a Model\n","\n","1. **Data** — What are the inputs and outputs?\n","2. **Architecture** — What kind of model are we using? (e.g., linear function, softmax layer)\n","3. **Loss Function** — How do we measure how wrong the predictions are?\n","4. **Training Algorithm** — How do we improve the model? (usually with gradient descent)\n","\n","\n","### Linear Regression\n","\n","**Goal**: Predict a single number (e.g., a house price)\n","\n","#### 1. Data  \n","\n","- Input: vector `x` of features (e.g., size, location)  \n","- Output: scalar `y` (e.g., price)\n","\n","#### 2. Architecture  \n","\n","A simple linear model:\n","\n","```\n","ŷ = w · x + b\n","```\n","\n","Where:\n","- `w` is a weight vector  \n","- `b` is a scalar bias  \n","- `ŷ` is the predicted value\n","\n","#### 3. Loss Function  \n","\n","We use **Mean Squared Error (MSE)**:\n","\n","```\n","L = (ŷ - y)²\n","```\n","\n","#### 4. Training Algorithm  \n","\n","Use gradient descent to minimize the loss by updating `w` and `b`.\n","\n","### Logistic Regression\n","\n","**Goal**: Predict a **class**, not a number\n","\n","#### 1. Data  \n","\n","- Input: vector `x` of features (e.g., tokenized message)  \n","- Output: vector `y` of class labels\n","\n","#### 2. Architecture  \n","\n","A linear model plus an activation function.\n","\n","**Binary Classification:**\n","\n","```\n","z = w · x + b  \n","ŷ = sigmoid(z)\n","```\n","\n","**Multiclass Classification:**\n","\n","```\n","z = w · x + b  \n","ŷ = softmax(z)\n","```\n","\n","Where:\n","- `w` is now a **matrix** (one row of weights per class)  \n","- `ŷ` is a **vector of probabilities**, one per class\n","- `softmax` or `sigmoid` is the activation function\n","\n","#### 3. Loss Function  \n","\n","We use **Cross-Entropy Loss**:\n","\n","```\n","L = -log(ŷ_y)\n","```\n","\n","Where `ŷ_y` is the predicted probability for the true class `y`.\n","\n","#### 4. Training Algorithm  \n","\n","Use gradient descent to minimize the loss by updating `w` and `b`.\n","\n","## From Logistic Regression to Neural Networks\n","\n","So far, we’ve seen how a **logistic regression model** can take an input vector `x`, apply a linear transformation, and then use **softmax** to produce a prediction.\n","\n","But what if the data isn’t linearly separable? What if we want the model to learn **interactions between features**, or **nonlinear patterns**?\n","\n","To solve that, we introduce the next step in our journey:\n","\n","**Multi-Layer Networks (Neural Networks)**\n","\n","### Key Idea\n","\n","Add **layers** of computation between the input and the output.\n","\n","These layers apply **nonlinear functions** to the data — giving the model more flexibility and power to learn complex patterns.\n","\n","### Neural Network Architecture\n","\n","The simplest kind of neural network is called a **feedforward network** or **multi-layer perceptron (MLP)**.\n","\n","It looks like this:\n","\n","```\n","input x\n","   ↓\n","Linear: z₁ = W₁ · x + b₁\n","   ↓\n","Nonlinearity: h = ReLU(z₁)\n","   ↓\n","Linear: z₂ = W₂ · h + b₂\n","   ↓\n","Softmax: ŷ = softmax(z₂)\n","```\n","\n","Each step transforms the data a bit more, allowing the model to build up increasingly abstract representations.\n","\n","### Hidden Layers\n","\n","A **hidden layer** is any layer that comes between the input and the output.\n","\n","- It has its own weights and biases (`W₁`, `b₁`)\n","- It applies a **nonlinear activation function** like **ReLU**, **tanh**, or **sigmoid**\n","- It produces a new internal representation `h`, which becomes the input to the next layer\n","\n","### ReLU Activation Function\n","\n","One of the most popular activation functions is **ReLU (Rectified Linear Unit)**:\n","\n","```\n","ReLU(z) = max(0, z)\n","```\n","\n","Why ReLU?\n","\n","- It introduces **nonlinearity**, so the model can learn more than just lines or planes\n","- It's **simple to compute**\n","- It helps with **gradient flow** in deeper networks\n","\n","### Final Layer: Softmax\n","\n","As before, we use **softmax** at the output to turn raw scores into probabilities:\n","\n","```\n","ŷ = softmax(W₂ · h + b₂)\n","```\n","\n","Where:\n","\n","- `h` is the hidden layer output  \n","- `W₂`, `b₂` are the weights and biases of the final layer  \n","- `ŷ` is a probability distribution over classes\n","\n","### Updated 4 Ingredients\n","\n","We’re still following the same recipe — just with more expressive power:\n","\n","1. **Data** — still the same input-output pairs  \n","2. **Architecture** — now has **multiple layers** and **nonlinearities**  \n","3. **Loss Function** — still **cross-entropy** for classification  \n","4. **Training Algorithm** — still **gradient descent**, now applied to **every layer**\n","\n","We’ll need to compute gradients for all weights using a technique called **backpropagation** — a generalization of the chain rule from calculus.\n","\n","### Summary\n","\n","- A **neural network** is a stack of linear layers and nonlinear activations  \n","- Each layer transforms the data into a new space  \n","- Nonlinear activations (like ReLU) give the model the power to learn complex patterns  \n","- At the output, we use softmax to predict class probabilities  \n","- Training is done using gradient descent — just like logistic regression\n","\n","## Backpropagation: How Neural Networks Learn\n","\n","Now that we've introduced multi-layer networks, the big question is:\n","\n","**How do we train all these layers?**\n","\n","In logistic regression, we computed the gradient of the loss with respect to the weights and biases using the chain rule. In a neural network, we do the same thing — just across **multiple layers**.\n","\n","This process is called **backpropagation**.\n","\n","### What Is Backpropagation?\n","\n","**Backpropagation** is a method for computing the gradient of the loss with respect to **every parameter in the network**.\n","\n","It’s based on two ideas:\n","\n","1. **Chain Rule** (from calculus):  \n","   If a function is made of multiple parts, we can compute its derivative by multiplying the derivatives of each part.\n","\n","2. **Reusing Intermediate Results**:  \n","   Instead of recomputing everything from scratch, we compute gradients layer by layer, starting from the output and working backward.\n","\n","That’s why it’s called **back**propagation — we go **backwards** through the network.\n","\n","### The 4 Ingredients (Again)\n","\n","Let’s revisit the model-building recipe:\n","\n","1. **Data** — inputs `x`, targets `y`\n","2. **Architecture** — multiple layers with weights and activations\n","3. **Loss Function** — measures how wrong our prediction is\n","4. **Training Algorithm** — this is where backpropagation lives\n","\n","Backpropagation is how we implement gradient descent for **multi-layer networks**.\n","\n","### Backprop: Step by Step\n","\n","For a simple neural network like this:\n","\n","```\n","x → Linear → ReLU → Linear → Softmax → Loss\n","```\n","\n","Backpropagation proceeds as follows:\n","\n","1. **Forward pass**:  \n","   Compute predictions and loss just like normal.\n","\n","2. **Backward pass**:  \n","   - Start with the gradient of the loss (∂L/∂ŷ)\n","   - Use the **chain rule** to compute gradients for the last layer  \n","     (e.g., ∂L/∂W₂ and ∂L/∂b₂)\n","   - Pass the error **backward** through the ReLU  \n","   - Continue to compute gradients for the first layer  \n","     (e.g., ∂L/∂W₁ and ∂L/∂b₁)\n","\n","3. **Update weights**:  \n","   Use gradient descent to adjust all weights and biases.\n","\n","### Why It Works\n","\n","Each layer of the network is just a function — usually linear followed by a nonlinearity. Because we know how to differentiate each part, we can use the chain rule to compute the full gradient.\n","\n","This allows us to **learn all the weights in the network**, no matter how many layers there are.\n","\n","### Summary\n","\n","- Backpropagation is a generalization of gradient descent for multi-layer models\n","- It uses the **chain rule** to compute gradients from output to input\n","- It allows us to train all layers of a neural network — not just the last one\n","- It’s the foundation of modern deep learning\n","\n","You don’t need to memorize the math — the key idea is that we can compute gradients for each layer automatically and use them to improve the model.\n","\n","## What Is PyTorch?\n","\n","**PyTorch** is a popular open-source deep learning library developed by **Meta (Facebook AI Research)**. It provides flexible tools for:\n","\n","- Working with **tensors** (multi-dimensional arrays)\n","- Building **neural networks**\n","- Running models on **GPUs** for fast computation\n","- Automatically computing **gradients** using a system called **autograd**\n","\n","PyTorch is used by researchers, engineers, and companies around the world — from small prototypes to large-scale production systems.\n","\n","### Why Are We Using It Now?\n","\n","Building models from scratch helped us:\n","\n","- Understand how predictions, loss, gradients, and updates actually work\n","- See how matrix math underpins all learning\n","\n","But in practice, almost no one writes models completely from scratch.  \n","Instead, people use **high-level libraries** like PyTorch and Google's TensorFlow.\n","\n","These tools save time, reduce bugs, and make it easy to scale up to larger models.\n","\n","> From here on, we'll use PyTorch to build and train our neural networks — but now you’ll understand exactly what’s going on under the hood.\n","\n","## SMS Spam Collection\n","\n","We’re returning to the **SMS Spam Collection** — a dataset we’ve used a few times already to explore text classification.\n","\n","Each row in the dataset contains:\n","- A **label**: either `\"spam\"` or `\"ham\"` (not spam)\n","- A **message**: the text content of an SMS\n","\n","Previously, we used:\n","- **Naive Bayes** to classify messages using word counts\n","- **Logistic Regression** (from scratch) with TF-IDF vectors\n","\n","This time, we’ll build a **multi-layer neural network** to perform the same task — using **PyTorch** to manage tensors, layers, gradients, and optimization.\n","\n","> Same dataset, but now a more powerful model — and a much more scalable framework.\n","\n","## Preparing the Data\n","\n","Before we can train a neural network, we need to turn our SMS messages into numeric input vectors that PyTorch can understand. In this section, we:\n","\n","1. **Load the dataset**\n","2. **Split into training and test sets**\n","3. **Tokenize the messages**\n","4. **Compute TF‑IDF scores**\n","5. **Convert messages into PyTorch tensors**\n","6. **Wrap everything in a Dataset and DataLoader**\n","\n","### Step 1: Load the Dataset\n","\n","We use `pandas` to read the SMS Spam Collection file from a URL. Each row contains a label (`\"spam\"` or `\"ham\"`) and a message.\n","\n"],"id":"FrprP7b32am5"},{"cell_type":"code","execution_count":1,"metadata":{"id":"TEkygNgm2am8","executionInfo":{"status":"ok","timestamp":1752528705248,"user_tz":240,"elapsed":1337,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv(\n","    \"https://wd13ca.github.io/BAN200-Summer-2025/SMSSpamCollection.txt\",\n","    sep=\"\\t\", header=None, names=[\"label\", \"message\"]\n",")\n"],"id":"TEkygNgm2am8"},{"cell_type":"markdown","metadata":{"id":"bTNbCPzw2am9"},"source":["\n","### Step 2: Train-Test Split\n","\n","We randomly split the dataset: 80% for training, 20% for testing.\n","\n"],"id":"bTNbCPzw2am9"},{"cell_type":"code","execution_count":2,"metadata":{"id":"9rxfUuM32am9","executionInfo":{"status":"ok","timestamp":1752528707444,"user_tz":240,"elapsed":2187,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=13)\n"],"id":"9rxfUuM32am9"},{"cell_type":"markdown","metadata":{"id":"xXrot-4h2am_"},"source":["\n","### Step 3: Tokenize the Text\n","\n","We define a simple tokenizer that:\n","- Lowercases the text\n","- Removes stop words (like \"the\", \"and\", etc.)\n","- Extracts word tokens using regular expressions\n","\n"],"id":"xXrot-4h2am_"},{"cell_type":"code","execution_count":3,"metadata":{"id":"j17SVs6z2am_","executionInfo":{"status":"ok","timestamp":1752528707461,"user_tz":240,"elapsed":16,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import re\n","from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n","\n","def tokenize(text):\n","    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n","    return [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n"],"id":"j17SVs6z2am_"},{"cell_type":"markdown","metadata":{"id":"ocZrDBvW2am_"},"source":["\n","### Step 4: Compute IDF Scores\n","\n","We compute the **inverse document frequency (IDF)** of each word in the training set. Words that appear in many messages get lower weights; rare words get higher weights.\n","\n"],"id":"ocZrDBvW2am_"},{"cell_type":"code","execution_count":4,"metadata":{"id":"wp-XWP7R2anA","executionInfo":{"status":"ok","timestamp":1752528707628,"user_tz":240,"elapsed":166,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import math\n","\n","N = len(train_df)\n","doc_freq = {}\n","for msg in train_df[\"message\"]:\n","    for tok in set(tokenize(msg)):\n","        doc_freq[tok] = doc_freq.get(tok, 0) + 1\n","\n","idf = {tok: math.log(N / df) for tok, df in doc_freq.items()}\n"],"id":"wp-XWP7R2anA"},{"cell_type":"markdown","metadata":{"id":"sFdzqiBb2anA"},"source":["\n","### Step 5: Vectorize Each Message\n","\n","Finally, we create a tokenizer that converts a message into a dense PyTorch tensor. Each element of the vector corresponds to a word in the vocabulary, weighted by its TF‑IDF score.\n","\n"],"id":"sFdzqiBb2anA"},{"cell_type":"code","execution_count":5,"metadata":{"id":"z9dzfyg42anB","executionInfo":{"status":"ok","timestamp":1752528707629,"user_tz":240,"elapsed":4,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["vocab = [word for word, _ in sorted(doc_freq.items(), key=lambda item: item[1], reverse=True)]\n","word2idx = {w: i for i, w in enumerate(vocab)}\n","\n","def vectorize(message):\n","    vec = torch.zeros(len(vocab))\n","    for tok in tokenize(message):\n","        if tok in idf:\n","            vec[word2idx[tok]] += idf[tok]\n","    return vec\n"],"id":"z9dzfyg42anB"},{"cell_type":"markdown","metadata":{"id":"eL9Y6KUU2anB"},"source":["\n","### Step 6: Create a Dataset and DataLoader\n","\n","To use PyTorch effectively, we wrap our data in a `Dataset` class and feed it to a `DataLoader`. This handles batching and shuffling automatically.\n","\n","\n"],"id":"eL9Y6KUU2anB"},{"cell_type":"code","execution_count":6,"metadata":{"id":"I6fVrpTF2anB","executionInfo":{"status":"ok","timestamp":1752528720645,"user_tz":240,"elapsed":13018,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","label2index = {'spam': 0, 'ham': 1}\n","\n","class SMSDataset(Dataset):\n","    def __init__(self, df):\n","        self.x = torch.stack([vectorize(m) for m in df[\"message\"]])\n","        self.y = torch.tensor([label2index[lbl] for lbl in df[\"label\"]])\n","    def __len__(self):\n","        return len(self.y)\n","    def __getitem__(self, idx):\n","        return self.x[idx], self.y[idx]\n","\n","batch_size = 64\n","train_loader = DataLoader(SMSDataset(train_df), batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(SMSDataset(test_df),  batch_size=batch_size)\n"],"id":"I6fVrpTF2anB"},{"cell_type":"markdown","metadata":{"id":"06awDppv2anC"},"source":["\n","> At this point, we have a working PyTorch pipeline: raw text → TF‑IDF vector → PyTorch tensor → mini-batches for training.\n","\n","## Defining the Model\n","\n","Now that our data is ready, let’s define the neural network we’ll use to classify SMS messages.\n","\n","We’ll build a **two-layer feedforward neural network**:\n","\n","1. A **linear layer** that maps from the input size to a hidden dimension  \n","2. A **ReLU activation** to introduce nonlinearity  \n","3. A second **linear layer** that maps from the hidden dimension to 2 output units (spam or ham)\n","\n","### Why 2 Outputs?\n","\n","Because we encoded the labels as `[1.0, 0.0]` for spam and `[0.0, 1.0]` for ham, the model must output two numbers — one for each class. We'll later apply **softmax** or **cross-entropy loss** to turn those into probabilities.\n","\n","### PyTorch Model Definition\n","\n","We’ll use `torch.nn.Sequential` to stack the layers.\n","\n"],"id":"06awDppv2anC"},{"cell_type":"code","execution_count":7,"metadata":{"id":"MWdz-T912anC","executionInfo":{"status":"ok","timestamp":1752528720647,"user_tz":240,"elapsed":10,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class TwoLayerNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n"],"id":"MWdz-T912anC"},{"cell_type":"markdown","metadata":{"id":"5eFbH1D02anC"},"source":["\n","### Instantiate the Model\n","\n","We define:\n","- `input_dim`: the size of the TF‑IDF vector (i.e. vocab size)\n","- `hidden_dim`: number of hidden units (you can tune this)\n","- `output_dim`: 2 (for spam and ham)\n","\n"],"id":"5eFbH1D02anC"},{"cell_type":"code","execution_count":8,"metadata":{"id":"5S08hm3d2anC","executionInfo":{"status":"ok","timestamp":1752528720647,"user_tz":240,"elapsed":9,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["input_dim = len(vocab)\n","hidden_dim = 100\n","output_dim = 2\n","\n","model = TwoLayerNet(input_dim, hidden_dim, output_dim)\n"],"id":"5S08hm3d2anC"},{"cell_type":"markdown","metadata":{"id":"YKIRlJdB2anC"},"source":["\n","> This model has learnable weights in both layers and can learn complex patterns in the input — much more powerful than logistic regression.\n","\n","Next, we’ll train this model using gradient descent.\n","\n","## Training the Model\n","\n","To train the model, we need two key components:\n","\n","1. A **loss function** to measure how wrong the predictions are  \n","2. An **optimizer** to update the model’s weights using gradients\n","\n","We’ll use:\n","- `nn.CrossEntropyLoss()` — combines softmax + log-loss\n","- `torch.optim.Adam` — a good default optimizer\n","\n"],"id":"YKIRlJdB2anC"},{"cell_type":"code","execution_count":9,"metadata":{"id":"0vb2pp2Z2anD","executionInfo":{"status":"ok","timestamp":1752528728112,"user_tz":240,"elapsed":7468,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"],"id":"0vb2pp2Z2anD"},{"cell_type":"markdown","metadata":{"id":"_7NHE_Ij2anD"},"source":["\n","Now we train the model over multiple epochs:\n","\n"],"id":"_7NHE_Ij2anD"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5by_nXo52anD","executionInfo":{"status":"ok","timestamp":1752528738801,"user_tz":240,"elapsed":10701,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"d2761967-7448-4d2f-806d-05ee192b7b29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 0.2922, Acc = 90.78% | Test Loss = 0.0826, Acc = 98.39%\n","Epoch 2: Train Loss = 0.0338, Acc = 99.66% | Test Loss = 0.0614, Acc = 98.65%\n","Epoch 3: Train Loss = 0.0110, Acc = 99.96% | Test Loss = 0.0667, Acc = 98.57%\n","Epoch 4: Train Loss = 0.0058, Acc = 99.98% | Test Loss = 0.0719, Acc = 98.48%\n","Epoch 5: Train Loss = 0.0035, Acc = 99.98% | Test Loss = 0.0783, Acc = 98.48%\n","Epoch 6: Train Loss = 0.0025, Acc = 99.98% | Test Loss = 0.0842, Acc = 98.48%\n","Epoch 7: Train Loss = 0.0018, Acc = 99.98% | Test Loss = 0.0889, Acc = 98.48%\n","Epoch 8: Train Loss = 0.0014, Acc = 99.98% | Test Loss = 0.0932, Acc = 98.30%\n","Epoch 9: Train Loss = 0.0012, Acc = 99.98% | Test Loss = 0.0977, Acc = 98.30%\n","Epoch 10: Train Loss = 0.0010, Acc = 99.98% | Test Loss = 0.1022, Acc = 98.30%\n"]}],"source":["epochs = 10\n","\n","for epoch in range(epochs):\n","    # --- Training phase ---\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for xb, yb in train_loader:\n","        optimizer.zero_grad()\n","        logits = model(xb)\n","        loss = criterion(logits, yb)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * len(yb)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == yb).sum().item()\n","        total += len(yb)\n","\n","    avg_train_loss = total_loss / total\n","    train_acc = correct / total\n","\n","    # --- Evaluation phase ---\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for xb, yb in test_loader:\n","            logits = model(xb)\n","            loss = criterion(logits, yb)\n","            test_loss += loss.item() * len(yb)\n","            preds = logits.argmax(dim=1)\n","            correct += (preds == yb).sum().item()\n","            total += len(yb)\n","\n","    avg_test_loss = test_loss / total\n","    test_acc = correct / total\n","\n","    print(f\"Epoch {epoch+1}: \"\n","          f\"Train Loss = {avg_train_loss:.4f}, Acc = {train_acc:.2%} | \"\n","          f\"Test Loss = {avg_test_loss:.4f}, Acc = {test_acc:.2%}\")\n"],"id":"5by_nXo52anD"},{"cell_type":"markdown","metadata":{"id":"G-aM2HjM2anE"},"source":["\n","After training, we’ll evaluate the model’s accuracy on the test set.\n","\n","## Overfitting and Early Stopping\n","\n","As we build more powerful models — especially neural networks with multiple layers — we also increase the risk of **overfitting**.\n","\n","### What Is Overfitting?\n","\n","**Overfitting** happens when a model learns to perform very well on the **training data**, but fails to generalize to **new, unseen data**. In other words, the model starts to memorize the training set instead of learning useful patterns.\n","\n","### Why Does It Happen?\n","\n","Overfitting is more likely when:\n","- The model has **many parameters** (e.g., deep or wide networks)\n","- The dataset is **small** or **noisy**\n","- Training runs for **too many epochs**\n","\n","The model becomes so flexible that it can \"explain\" the training data perfectly — including the noise — but it performs poorly on the test set.\n","\n","You’ll often see this pattern:\n","\n","- **Training loss keeps decreasing**\n","- **Validation (test) loss starts increasing**\n","\n","This is a clear sign of overfitting.\n","\n","### One Solution: Early Stopping\n","\n","**Early stopping** is a simple and effective way to avoid overfitting.\n","\n","Here’s how it works:\n","- During training, monitor performance on the **validation set**\n","- If validation loss stops improving for several epochs in a row, **stop training early**\n","- Keep the model from the epoch with the **lowest validation loss**\n","\n","Early stopping helps prevent the model from \"going too far\" and starting to memorize the training data.\n","\n","> Overfitting is a sign that your model is too powerful for your data — regularization, more data, or simpler models can help, but early stopping is often the easiest place to start.\n","\n","\n","## Evaluate the Model\n","\n","### Predict Function\n","\n","Let's create a `predict()` function that takes an unlabeled message and returns a predicted label:\n","\n"],"id":"G-aM2HjM2anE"},{"cell_type":"code","execution_count":11,"metadata":{"id":"Tket7oDc2anE","executionInfo":{"status":"ok","timestamp":1752528738811,"user_tz":240,"elapsed":3,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}}},"outputs":[],"source":["def predict(message):\n","    \"\"\"\n","    Predict the class label for a raw input message (string).\n","    - message: the input message (e.g., \"Free entry now!!!\")\n","\n","    Returns:\n","        predicted_label: the class label with highest probability\n","    \"\"\"\n","    model.eval()\n","    with torch.no_grad():\n","        vec = vectorize(message).unsqueeze(0)  # add batch dimension\n","        logits = model(vec)                   # raw scores (1 x num_classes)\n","        predicted_class = logits.argmax(dim=1).item()\n","    return list(label2index.keys())[list(label2index.values()).index(predicted_class)]\n","\n"],"id":"Tket7oDc2anE"},{"cell_type":"markdown","metadata":{"id":"uu8ThtL82anE"},"source":["\n","### Evaluate Model Accuracy on the Test Set\n","\n","Now that we have a working `predict()` function, we’ll apply it to every message in the test set.\n","\n","Then we’ll compare the predicted labels to the actual labels and calculate the model’s accuracy.\n","\n"],"id":"uu8ThtL82anE"},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ab3tNO_2anE","executionInfo":{"status":"ok","timestamp":1752528739790,"user_tz":240,"elapsed":973,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"8d486fe9-6233-4f22-8ca2-90e94ce1b15a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 98.30%\n"]}],"source":["# Predict all messages in the test set\n","predictions = []\n","\n","for _, row in test_df.iterrows():\n","    message = row[\"message\"]\n","    prediction = predict(message)\n","    predictions.append(prediction)\n","\n","# Actual labels\n","actual = test_df[\"label\"].tolist()\n","\n","# Compute accuracy\n","correct = sum([pred == truth for pred, truth in zip(predictions, actual)])\n","accuracy = correct / len(test_df)\n","\n","print(f\"Accuracy: {accuracy:.2%}\")\n"],"id":"2Ab3tNO_2anE"},{"cell_type":"markdown","metadata":{"id":"sbF1tVTB2anE"},"source":["\n","### Confusion Matrix\n","\n","Now let's take a look at the confusion matrix:\n","\n"],"id":"sbF1tVTB2anE"},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmlF38Mi2anF","executionInfo":{"status":"ok","timestamp":1752528739796,"user_tz":240,"elapsed":16,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"eba6c235-8850-44ae-8735-5f3b0017dcb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n","               Predicted\n","             | spam | ham \n","Actual spam  |   136 |   18\n","Actual ham   |     1 |  960\n"]}],"source":["from sklearn.metrics import confusion_matrix\n","\n","# Generate confusion matrix\n","cm = confusion_matrix(actual, predictions, labels=[\"spam\", \"ham\"])\n","\n","# Display as a readable table\n","print(\"Confusion Matrix:\")\n","print(f\"               Predicted\")\n","print(f\"             | spam | ham \")\n","print(f\"Actual spam  |  {cm[0][0]:4} | {cm[0][1]:4}\")\n","print(f\"Actual ham   |  {cm[1][0]:4} | {cm[1][1]:4}\")\n"],"id":"jmlF38Mi2anF"},{"cell_type":"markdown","metadata":{"id":"LbyaWi4u2anF"},"source":["\n","### Precision, Recall, and F1-Score\n","\n","Here are the precision, recall, and F1-scores:\n","\n"],"id":"LbyaWi4u2anF"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcmlnJHe2anF","executionInfo":{"status":"ok","timestamp":1752528739818,"user_tz":240,"elapsed":29,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"167ebc26-6aca-46cf-a11b-94d6d5af78cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        spam       0.98      1.00      0.99       961\n","         ham       0.99      0.88      0.93       154\n","\n","    accuracy                           0.98      1115\n","   macro avg       0.99      0.94      0.96      1115\n","weighted avg       0.98      0.98      0.98      1115\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(actual, predictions, target_names=[\"spam\", \"ham\"]))\n"],"id":"bcmlnJHe2anF"},{"cell_type":"markdown","metadata":{"id":"rI63yJ542anF"},"source":["\n","### Error Analysis\n","\n","Let’s look at some misclassified messages — where the model's prediction didn't match the true label.\n","\n","This helps us understand:\n","\n","- Where the model is confused\n","- Whether certain types of spam are being missed\n","- If it’s too aggressive (labeling ham as spam)\n","\n"],"id":"rI63yJ542anF"},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFU2rBGT2anF","executionInfo":{"status":"ok","timestamp":1752528739887,"user_tz":240,"elapsed":68,"user":{"displayName":"Will Dick","userId":"09439979286280420660"}},"outputId":"0ec0743f-ed4f-4c17-cf54-55b15d0efdd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Ur balance is now £500. Ur next question is: Who sang 'Uptown Girl' in the 80's ? 2 answer txt ur ANSWER to 83600. Good luck!\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Thanks for the Vote. Now sing along with the stars with Karaoke on your mobile. For a FREE link just reply with SING now.\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   This message is brought to you by GMW Ltd. and is not connected to the\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Dear Voucher Holder 2 claim your 1st class airport lounge passes when using Your holiday voucher call 08704439680. When booking quote 1st class x 2\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Reply with your name and address and YOU WILL RECEIVE BY POST a weeks completely free accommodation at various global locations www.phb1.com ph:08700435505150p\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   500 free text msgs. Just text ok to 80488 and we'll credit your account\n","\n","--- Misclassified Message ---\n","Actual:    ham\n","Predicted: spam\n","Message:   May b approve panalam...but it should have more posts..\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Please call Amanda with regard to renewing or upgrading your current T-Mobile handset free of charge. Offer ends today. Tel 0845 021 3680 subject to T's and C's\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Hi I'm sue. I am 20 years old and work as a lapdancer. I love sex. Text me live - I'm i my bedroom now. text SUE to 89555. By TextOperator G2 1DA 150ppmsg 18+\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Check Out Choose Your Babe Videos @ sms.shsex.netUN fgkslpoPW fgkslpo\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Missed call alert. These numbers called but left no message. 07008009200\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Hi if ur lookin 4 saucy daytime fun wiv busty married woman Am free all next week Chat now 2 sort time 09099726429 JANINExx Calls£1/minMobsmoreLKPOBOX177HP51FL\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Goal! Arsenal 4 (Henry, 7 v Liverpool 2 Henry scores with a simple shot from 6 yards from a pass by Bergkamp to give Arsenal a 2 goal margin after 78 mins.\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Will u meet ur dream partner soon? Is ur career off 2 a flyng start? 2 find out free, txt HORO followed by ur star sign, e. g. HORO ARIES\n","\n","--- Misclassified Message ---\n","Actual:    spam\n","Predicted: ham\n","Message:   Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?\n"]}],"source":["# Show the first 10 misclassified messages\n","for i in range(len(test_df)):\n","    if predictions[i] != actual[i]:\n","        print(f\"\\n--- Misclassified Message ---\")\n","        print(f\"Actual:    {actual[i]}\")\n","        print(f\"Predicted: {predictions[i]}\")\n","        print(f\"Message:   {test_df.iloc[i]['message']}\")\n"],"id":"FFU2rBGT2anF"},{"cell_type":"markdown","metadata":{"id":"3FvkzBQ92anF"},"source":["\n","## Exercises\n","\n","Try these exercises to deepen your understanding of neural network architecture and PyTorch:\n","\n","1. Change the Number of Hidden Units\n","2. Add an Extra Layer\n","3. Remove the Hidden Layer\n","4. Try a New Dataset\n","\n"],"id":"3FvkzBQ92anF"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.x"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}