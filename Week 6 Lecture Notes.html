<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Week 6 Lecture Notes</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>


</head>

<body>

<h1 id="toc_0">Week 6 Lecture Notes</h1>

<p>This week, we will continue our introduction of neural networks. Today, we will be building a logistic regression on the SMS Spam Collection using a neural network approach. This will set the stage for the more advanced models we will be working with after study week.</p>

<h2 id="toc_1">Multiclass Logistic Regression</h2>

<p>So far, we’ve looked at <strong>linear regression</strong>, where the model predicts a <strong>single number</strong>. Now let’s extend this idea to a classification task, where the goal is to predict <strong>which category</strong> something belongs to.</p>

<h3 id="toc_2">From Regression to Classification</h3>

<p>In classification, instead of predicting a single continuous value, we want to predict one of several <strong>classes</strong>. </p>

<p>For example, we might want to classify a customer email as:</p>

<ul>
<li><code>0</code> = Billing<br></li>
<li><code>1</code> = Sales<br></li>
<li><code>2</code> = Tech Support</li>
</ul>

<h3 id="toc_3">Outputs as a Vector</h3>

<p>Instead of outputting a single number <code>ŷ</code>, the model now outputs a <strong>vector of values</strong> — one for each class.</p>

<p>For example, the model might output a vector like this:</p>

<div><pre><code class="language-none">ŷ = [0.1, 0.7, 0.2]</code></pre></div>

<p>We can interpret this as:</p>

<ul>
<li>10% chance it&#39;s Billing<br></li>
<li>70% chance it&#39;s Sales<br></li>
<li>20% chance it&#39;s Tech Support<br></li>
</ul>

<p>And we would classify this example as <strong>Sales</strong>, since it has the highest score.</p>

<h3 id="toc_4">Model Equation</h3>

<p>Our model equation is now:</p>

<div><pre><code class="language-none">ŷ = softmax(w · x + b)</code></pre></div>

<p>Let’s break it down:</p>

<ul>
<li><code>x</code> is the <strong>input vector</strong> (with <code>n</code> features)</li>
<li><code>w</code> is the <strong>weight matrix</strong>, with shape <code>k × n</code><br>
(one row of weights for each of the <code>k</code> classes)</li>
<li><code>b</code> is the <strong>bias vector</strong>, with one bias per class</li>
<li>The result <code>w · x + b</code> is a <strong>vector of raw scores</strong> (called <strong>logits</strong>)</li>
<li>We apply a function called <strong>softmax</strong> to turn those scores into <strong>probabilities</strong></li>
</ul>

<h3 id="toc_5">Softmax: From Scores to Probabilities</h3>

<p>Softmax is a type of <strong>activation function</strong> — a function applied at the output of a model to shape or interpret the result in some useful way.</p>

<p>The softmax function takes in a vector of raw scores and returns a vector of probabilities that sum to 1. It’s defined as:</p>

<div><pre><code class="language-none">softmax(zᵢ) = exp(zᵢ) / Σⱼ exp(zⱼ)</code></pre></div>

<p>Where:
- <code>zᵢ</code> is the raw score for class <code>i</code> (the <code>i</code>th element of the logits vector)
- <code>exp(zᵢ)</code> makes all the scores positive and amplifies larger ones
- The denominator ensures the outputs sum to 1</p>

<blockquote>
<p>This lets us interpret the outputs as a <strong>probability distribution</strong> over the classes.</p>
</blockquote>

<h3 id="toc_6">Why Is <code>w</code> a Matrix?</h3>

<p>In linear regression, we had:</p>

<div><pre><code class="language-none">ŷ = w · x + b</code></pre></div>

<p>Here, <code>w</code> was a vector (1D), and <code>ŷ</code> was a single number.</p>

<p>But in classification, we want <strong>one output per class</strong>, so we need <strong>a different set of weights for each class</strong>. That means <code>w</code> becomes a <strong>matrix</strong>:</p>

<ul>
<li>Each <strong>row</strong> of <code>w</code> corresponds to one class</li>
<li>The dot product between that row and the input <code>x</code> gives a <strong>score</strong> for that class</li>
</ul>

<p>In effect, it’s like running <strong>multiple linear regressions in parallel</strong> — one for each class — and then using softmax to pick the most likely one.</p>

<h3 id="toc_7">Summary</h3>

<p>Multiclass logistic regression is a natural extension of linear regression:</p>

<ul>
<li>We go from a <strong>single output</strong> to a <strong>vector of outputs</strong></li>
<li>We go from a <strong>weight vector</strong> to a <strong>weight matrix</strong></li>
<li>We apply a <strong>nonlinear activation function</strong> (softmax) to interpret the outputs as probabilities</li>
</ul>

<h2 id="toc_8">Deriving the Gradient: Multiclass Logistic Regression</h2>

<p>We’ll now derive the gradient for a <strong>softmax classifier</strong> trained with <strong>cross-entropy loss</strong>.</p>

<h3 id="toc_9">Model Output</h3>

<p>The model computes:</p>

<div><pre><code class="language-none">z = w · x + b         # raw scores (logits)
ŷ = softmax(z)        # predicted probabilities</code></pre></div>

<p>Where:
- <code>x</code> is the input vector (length <code>n</code>)
- <code>w</code> is a weight matrix with shape <code>k × n</code> (k = number of classes)
- <code>b</code> is a bias vector (length <code>k</code>)
- <code>ŷ</code> is a vector of class probabilities (length <code>k</code>)</p>

<p>The softmax function for class <code>i</code> is:</p>

<div><pre><code class="language-none">ŷᵢ = exp(zᵢ) / sum_j exp(zⱼ)</code></pre></div>

<h3 id="toc_10">Loss Function: Cross-Entropy</h3>

<p>If the true class label is <code>y</code> (an integer from <code>0</code> to <code>k-1</code>), the loss is:</p>

<div><pre><code class="language-none">L = -log(ŷ_y)        # negative log of the predicted probability for the true class</code></pre></div>

<h3 id="toc_11">Goal</h3>

<p>We want to compute the gradients:</p>

<ul>
<li>∂L/∂wᵢⱼ — how the loss changes with respect to each weight</li>
<li>∂L/∂bᵢ  — how the loss changes with respect to each bias</li>
</ul>

<h3 id="toc_12">Gradient of the Loss</h3>

<p>We apply the chain rule. The derivative of the loss with respect to the logit <code>zᵢ</code> is:</p>

<div><pre><code class="language-none">∂L/∂zᵢ = ŷᵢ - 1   if i == y
∂L/∂zᵢ = ŷᵢ       otherwise</code></pre></div>

<p>This tells us that:</p>

<ul>
<li>For the <strong>correct class</strong>, we subtract 1 from the predicted probability</li>
<li>For all other classes, the gradient is just the predicted probability</li>
</ul>

<h3 id="toc_13">Final Gradients</h3>

<p>Now that we have ∂L/∂zᵢ, we compute the gradients with respect to the weights and biases:</p>

<div><pre><code class="language-none">∂L/∂wᵢⱼ = (ŷᵢ - 1[y = i]) * xⱼ
∂L/∂bᵢ  = (ŷᵢ - 1[y = i])</code></pre></div>

<p>Where <code>1[y = i]</code> is 1 if <code>i</code> is the correct class, otherwise 0.</p>

<h3 id="toc_14">Summary</h3>

<ul>
<li>The <strong>error</strong> for each class is:<br>
<code>
errorᵢ = ŷᵢ - 1[y = i]
</code></li>
<li>We multiply that error by each input feature to get the gradient:
<code>
dwᵢⱼ = errorᵢ * xⱼ
dbᵢ  = errorᵢ
</code></li>
</ul>

<p>This gives us the gradients we need to perform gradient descent on a multiclass softmax classifier.</p>

<h2 id="toc_15">SMS Spam Collection</h2>

<p>To see how logistic regressions work, let&#39;s try building one. We&#39;ll use the same dataset we used in Week 4. </p>

<h3 id="toc_16">Dataset, Tokenizer, and Vectorizer</h3>

<p>First, we&#39;ll download the data and split it into test and train datasets.</p>

<div><pre><code class="language-python"># Download the dataset
!wget https://wd13ca.github.io/BAN200-Summer-2025/SMSSpamCollection.txt

# Load it into a pandas DataFrame
import pandas as pd

df = pd.read_csv(&quot;SMSSpamCollection.txt&quot;, sep=&quot;\t&quot;, header=None, names=[&quot;label&quot;, &quot;message&quot;])

from sklearn.model_selection import train_test_split

# Split the dataset: 80% training, 20% testing
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Show the number of messages in each set
print(&quot;Training messages:&quot;, len(train_df))
print(&quot;Test messages:&quot;, len(test_df))</code></pre></div>

<p>Next, we&#39;ll recreate our tokenizer.</p>

<div><pre><code class="language-python">import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def tokenize(text):
    lowercase_text = text.lower()
    tokens = re.findall(r&#39;\b\w+\b&#39;, lowercase_text)
    return [t for t in tokens if t not in ENGLISH_STOP_WORDS]</code></pre></div>

<p>We&#39;ll also need a vectorizer. Let&#39;s use the one from Week 3. </p>

<div><pre><code class="language-python">from math import log

N = len(train_df) # total number of documents

doc_freq = {} # document frequency for each word
for _, row in train_df.iterrows():
  tokens = tokenize(row[&quot;message&quot;])
  unique_tokens = set(tokens) # only count once per document
  for token in unique_tokens:
      doc_freq[token] = doc_freq.get(token, 0) + 1

idf_dict = {}
for token, df in doc_freq.items():
    idf_dict[token] = log(N / df)

def vectorize_tf_idf(tokens):
    &quot;&quot;&quot;
    Takes a list of tokens and an IDF dictionary,
    returns a dictionary representing the TF-IDF vector.
    &quot;&quot;&quot;
    tf = {}
    for token in tokens:
        if token in tf:
            tf[token] += 1
        else:
            tf[token] = 1

    tf_idf = {}
    for token, freq in tf.items():
        if token in idf_dict:
            tf_idf[token] = freq * idf_dict[token]

    return tf_idf</code></pre></div>

<h3 id="toc_17">Defining a Model Class</h3>

<p>To build a multiclass classification model from scratch, we’ll use <strong>multiclass logistic regression</strong> with:</p>

<ul>
<li>A <strong>softmax activation function</strong> to produce probabilities</li>
<li>A <strong>cross-entropy loss</strong> to measure how wrong the predictions are</li>
<li><strong>Gradient descent</strong> to update the weights and bias</li>
</ul>

<p>We’ll define a Python class called <code>BoWLogisticClassifier</code> to encapsulate everything: prediction (<code>forward</code>), gradient computation (<code>backward</code>), and model parameters (<code>w</code> and <code>b</code>).</p>

<div><pre><code class="language-python">import math

class BoWLogisticClassifier:
    &quot;&quot;&quot;
    A sparse multiclass logistic regression model with softmax activation.
    Input vectors x are sparse dictionaries: {feature: value}.
    &quot;&quot;&quot;

    def __init__(self, vocab, classes):
        &quot;&quot;&quot;
        Initialize model with zero weights and zero biases.
        - vocab: a set of input feature names (e.g., words)
        - classes: a set of class labels (e.g., &#39;spam&#39;, &#39;ham&#39;)
        &quot;&quot;&quot;
        self.vocab = sorted(vocab)
        self.classes = sorted(classes)
        self.input_dim = len(self.vocab)
        self.output_dim = len(self.classes)

        # Map vocab and classes to index
        self.vocab_index = {word: i for i, word in enumerate(self.vocab)}
        self.class_index = {label: i for i, label in enumerate(self.classes)}

        # Initialize weights and biases
        self.w = [[0.0 for _ in range(self.input_dim)] for _ in range(self.output_dim)]
        self.b = [0.0 for _ in range(self.output_dim)]

    def softmax(self, logits):
        &quot;&quot;&quot;
        Apply softmax to logits for numerical stability.
        &quot;&quot;&quot;
        max_logit = max(logits)
        exp_scores = [math.exp(z - max_logit) for z in logits]
        sum_exp = sum(exp_scores)
        return [s / sum_exp for s in exp_scores]

    def forward(self, x_sparse):
        &quot;&quot;&quot;
        Compute prediction for sparse input x_sparse: dict {feature: value}.
        Returns probability distribution over classes.
        &quot;&quot;&quot;
        logits = []
        for k in range(self.output_dim):
            score = self.b[k]
            for word, value in x_sparse.items():
                if word in self.vocab_index:
                    j = self.vocab_index[word]
                    score += self.w[k][j] * value
            logits.append(score)
        return self.softmax(logits)

    def backward(self, x_sparse, y_true_label):
        &quot;&quot;&quot;
        Compute gradients w.r.t weights and biases using cross-entropy loss.
        - x_sparse: input features as dict {feature: value}
        - y_true_label: true class label (e.g., &#39;spam&#39;)

        Returns:
            dw: sparse gradient of weights (dict of dicts)
            db: dense gradient of biases (list)
        &quot;&quot;&quot;
        y_true = self.class_index[y_true_label]
        y_pred = self.forward(x_sparse)

        # Gradients
        dw = {k: {} for k in range(self.output_dim)}
        db = [0.0 for _ in range(self.output_dim)]

        for k in range(self.output_dim):
            error = y_pred[k] - (1 if k == y_true else 0)
            for word, value in x_sparse.items():
                if word in self.vocab_index:
                    j = self.vocab_index[word]
                    dw[k][word] = error * value
            db[k] = error

        return dw, db</code></pre></div>

<h3 id="toc_18">Training the Classifier</h3>

<p>Now let’s train our logistic regression model using gradient descent.</p>

<div><pre><code class="language-python">from collections import defaultdict
import random

# Build vocabulary and class labels from training set
all_tokens = [token for message in train_df[&quot;message&quot;] for token in tokenize(message)]
vocab = set(all_tokens)
classes = set(train_df[&quot;label&quot;])

# Initialize model
model = BoWLogisticClassifier(vocab=vocab, classes=classes)

# Prepare training data as (x_sparse, y_label) pairs
train_data = []
for _, row in train_df.iterrows():
    tokens = tokenize(row[&quot;message&quot;])
    x_sparse = vectorize_tf_idf(tokens)
    y_label = row[&quot;label&quot;]
    train_data.append((x_sparse, y_label))

# Training loop
def train(model, data, lr=0.1, epochs=5):
    for epoch in range(epochs):
        random.shuffle(data)
        total_loss = 0.0

        for x_sparse, y_label in data:
            y_true_idx = model.class_index[y_label]
            y_pred = model.forward(x_sparse)

            # Cross-entropy loss
            loss = -math.log(y_pred[y_true_idx] + 1e-12)
            total_loss += loss

            # Compute gradients
            dw, db = model.backward(x_sparse, y_label)

            # Update weights and biases
            for k in range(model.output_dim):
                for word, grad in dw[k].items():
                    j = model.vocab_index[word]
                    model.w[k][j] -= lr * grad
                model.b[k] -= lr * db[k]

        avg_loss = total_loss / len(data)
        print(f&quot;Epoch {epoch + 1}: Average Loss = {avg_loss:.4f}&quot;)

# Train the model
train(model, train_data, lr=0.1, epochs=10)</code></pre></div>

<h3 id="toc_19">Predict Function</h3>

<p>Let&#39;s create a <code>predict()</code> function that takes an unlabeled message and returns a predicted label:</p>

<div><pre><code class="language-python">def predict(message, model, tokenizer, vectorizer):
    &quot;&quot;&quot;
    Predict the class label for a raw input message (string).
    - message: the input message (e.g., &quot;Free entry now!!!&quot;)
    - tokenizer: a function that tokenizes the message
    - vectorizer: a function that turns tokens into sparse vector

    Returns:
        predicted_label: the class label with highest probability
    &quot;&quot;&quot;
    tokens = tokenizer(message)
    x_sparse = vectorizer(tokens)
    probs = model.forward(x_sparse)
    predicted_idx = probs.index(max(probs))
    return model.classes[predicted_idx]
</code></pre></div>

<h3 id="toc_20">Evaluate Model Accuracy on the Test Set</h3>

<p>Now that we have a working <code>predict()</code> function, we’ll apply it to every message in the test set.</p>

<p>Then we’ll compare the predicted labels to the actual labels and calculate the model’s accuracy.</p>

<div><pre><code class="language-python"># Predict all messages in the test set
predictions = []

for _, row in test_df.iterrows():
    message = row[&quot;message&quot;]
    prediction = predict(message,model,tokenize,vectorize_tf_idf)
    predictions.append(prediction)

# Actual labels
actual = test_df[&quot;label&quot;].tolist()

# Compute accuracy
correct = sum([pred == truth for pred, truth in zip(predictions, actual)])
accuracy = correct / len(test_df)

print(f&quot;Accuracy: {accuracy:.2%}&quot;)</code></pre></div>

<h3 id="toc_21">Confusion Matrix</h3>

<p>Now let&#39;s take a look at the confusion matrix:</p>

<div><pre><code class="language-python">from sklearn.metrics import confusion_matrix

# Generate confusion matrix
cm = confusion_matrix(actual, predictions, labels=[&quot;spam&quot;, &quot;ham&quot;])

# Display as a readable table
print(&quot;Confusion Matrix:&quot;)
print(f&quot;               Predicted&quot;)
print(f&quot;             | spam | ham &quot;)
print(f&quot;Actual spam  |  {cm[0][0]:4} | {cm[0][1]:4}&quot;)
print(f&quot;Actual ham   |  {cm[1][0]:4} | {cm[1][1]:4}&quot;)</code></pre></div>

<h3 id="toc_22">Precision, Recall, and F1-Score</h3>

<p>Here are the precision, recall, and F1-scores:</p>

<div><pre><code class="language-python">from sklearn.metrics import classification_report

print(classification_report(actual, predictions, target_names=[&quot;spam&quot;, &quot;ham&quot;]))</code></pre></div>

<h3 id="toc_23">Error Analysis</h3>

<p>Let’s look at some misclassified messages — where the model&#39;s prediction didn&#39;t match the true label.</p>

<p>This helps us understand:</p>

<ul>
<li>Where the model is confused</li>
<li>Whether certain types of spam are being missed</li>
<li>If it’s too aggressive (labeling ham as spam)</li>
</ul>

<div><pre><code class="language-python"># Show the first 10 misclassified messages
for i in range(len(test_df)):
    if predictions[i] != actual[i]:
        print(f&quot;\n--- Misclassified Message ---&quot;)
        print(f&quot;Actual:    {actual[i]}&quot;)
        print(f&quot;Predicted: {predictions[i]}&quot;)
        print(f&quot;Message:   {test_df.iloc[i][&#39;message&#39;]}&quot;)</code></pre></div>

<h2 id="toc_24">Logistic Regression vs. Naive Bayes: Why Logistic Regression Sometimes Performs Worse</h2>

<p>You may have noticed that our <strong>logistic regression model</strong> doesn&#39;t perform quite as well as the <strong>Naive Bayes model</strong> on the SMS Spam Collection dataset. This might seem surprising at first — after all, logistic regression is more flexible and learned its weights directly from the data. So what’s going on?</p>

<h3 id="toc_25">Why Logistic Regression Might Underperform</h3>

<p>Here are a few possible reasons:</p>

<ol>
<li><p><strong>Naive Bayes is surprisingly strong for text classification</strong><br>
Even though it makes strong independence assumptions (i.e. that words are independent given the class), those assumptions actually hold <em>reasonably well</em> in many text datasets. This makes Naive Bayes hard to beat, especially on short documents like SMS messages.</p></li>
<li><p><strong>Logistic regression is more sensitive to sparse features</strong><br>
Logistic regression must learn one weight per feature, and when the vocabulary is large and many features appear rarely (as in text), it can be harder for the model to learn stable estimates — especially with limited training data.</p></li>
<li><p><strong>No regularization in our implementation</strong><br>
Our logistic regression model is very basic: it doesn’t use techniques like <strong>L2 regularization</strong>, which can prevent overfitting and improve generalization — especially with high-dimensional sparse inputs like TF-IDF vectors.</p></li>
<li><p><strong>Naive Bayes builds in strong priors</strong><br>
Naive Bayes directly incorporates class priors (how common spam vs. ham messages are), and it’s especially effective when those priors are unbalanced. Logistic regression can learn this too, but it requires more data and care.</p></li>
</ol>

<h3 id="toc_26">Why Learn Logistic Regression Anyway?</h3>

<p>Even though Naive Bayes outperforms logistic regression <em>here</em>, logistic regression is still <strong>worth learning — and essential going forward</strong>:</p>

<ul>
<li>It introduces the idea of <strong>learning weights from data</strong>, not from counts or rules<br></li>
<li>It teaches the concept of <strong>gradient descent</strong>, which we’ll use to train more complex models<br></li>
<li>It prepares us for <strong>deep learning</strong>, where logistic regression is effectively the <strong>final layer</strong> of most neural networks used for classification<br></li>
<li>It scales better to complex, high-dimensional data once regularization and optimization improvements are added</li>
</ul>

<blockquote>
<p><strong>Key takeaway:</strong><br>
Logistic regression might not win on this dataset — but it’s a critical stepping stone toward more powerful and general models, like deep neural networks and transformers.</p>
</blockquote>

<h2 id="toc_27">Group Project Reminder</h2>

<p>Project Proposals are due next week. Class time will be used for groups to meet with the Professor to discuss their Project Proposals. </p>

<h2 id="toc_28">Midterm Reminder</h2>

<p>The Midterm will take place the week after Study Week. There will be no class that week. The Midterm will be online and open book. Students will have the entire week to complete it. It will cover all lecture material. (Material covered in the textbook will not be covered directly on the midterm - the textbook is just a resource to help you understand the code used in class.)</p>



<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
