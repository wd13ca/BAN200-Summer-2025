{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6 Lecture Notes\n",
        "\n",
        "This week, we will continue our introduction of neural networks. Today, we will be building a logistic regression on the SMS Spam Collection using a neural network approach. This will set the stage for the more advanced models we will be working with after study week.\n",
        "\n",
        "## Multiclass Logistic Regression\n",
        "\n",
        "So far, we\u2019ve looked at **linear regression**, where the model predicts a **single number**. Now let\u2019s extend this idea to a classification task, where the goal is to predict **which category** something belongs to.\n",
        "\n",
        "### From Regression to Classification\n",
        "\n",
        "In classification, instead of predicting a single continuous value, we want to predict one of several **classes**. \n",
        "\n",
        "For example, we might want to classify a customer email as:\n",
        "\n",
        "- `0` = Billing  \n",
        "- `1` = Sales  \n",
        "- `2` = Tech Support\n",
        "\n",
        "### Outputs as a Vector\n",
        "\n",
        "Instead of outputting a single number `\u0177`, the model now outputs a **vector of values** \u2014 one for each class.\n",
        "\n",
        "For example, the model might output a vector like this:\n",
        "\n",
        "```\n",
        "\u0177 = [0.1, 0.7, 0.2]\n",
        "```\n",
        "\n",
        "We can interpret this as:\n",
        "\n",
        "- 10% chance it's Billing  \n",
        "- 70% chance it's Sales  \n",
        "- 20% chance it's Tech Support  \n",
        "\n",
        "And we would classify this example as **Sales**, since it has the highest score.\n",
        "\n",
        "### Model Equation\n",
        "\n",
        "Our model equation is now:\n",
        "\n",
        "```\n",
        "\u0177 = softmax(w \u00b7 x + b)\n",
        "```\n",
        "\n",
        "Let\u2019s break it down:\n",
        "\n",
        "- `x` is the **input vector** (with `n` features)\n",
        "- `w` is the **weight matrix**, with shape `k \u00d7 n`  \n",
        "  (one row of weights for each of the `k` classes)\n",
        "- `b` is the **bias vector**, with one bias per class\n",
        "- The result `w \u00b7 x + b` is a **vector of raw scores** (called **logits**)\n",
        "- We apply a function called **softmax** to turn those scores into **probabilities**\n",
        "\n",
        "### Softmax: From Scores to Probabilities\n",
        "\n",
        "Softmax is a type of **activation function** \u2014 a function applied at the output of a model to shape or interpret the result in some useful way.\n",
        "\n",
        "The softmax function takes in a vector of raw scores and returns a vector of probabilities that sum to 1. It\u2019s defined as:\n",
        "\n",
        "```\n",
        "softmax(z\u1d62) = exp(z\u1d62) / \u03a3\u2c7c exp(z\u2c7c)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `z\u1d62` is the raw score for class `i` (the `i`th element of the logits vector)\n",
        "- `exp(z\u1d62)` makes all the scores positive and amplifies larger ones\n",
        "- The denominator ensures the outputs sum to 1\n",
        "\n",
        "> This lets us interpret the outputs as a **probability distribution** over the classes.\n",
        "\n",
        "### Why Is `w` a Matrix?\n",
        "\n",
        "In linear regression, we had:\n",
        "\n",
        "```\n",
        "\u0177 = w \u00b7 x + b\n",
        "```\n",
        "\n",
        "Here, `w` was a vector (1D), and `\u0177` was a single number.\n",
        "\n",
        "But in classification, we want **one output per class**, so we need **a different set of weights for each class**. That means `w` becomes a **matrix**:\n",
        "\n",
        "- Each **row** of `w` corresponds to one class\n",
        "- The dot product between that row and the input `x` gives a **score** for that class\n",
        "\n",
        "In effect, it\u2019s like running **multiple linear regressions in parallel** \u2014 one for each class \u2014 and then using softmax to pick the most likely one.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Multiclass logistic regression is a natural extension of linear regression:\n",
        "\n",
        "- We go from a **single output** to a **vector of outputs**\n",
        "- We go from a **weight vector** to a **weight matrix**\n",
        "- We apply a **nonlinear activation function** (softmax) to interpret the outputs as probabilities\n",
        "\n",
        "## Deriving the Gradient: Multiclass Logistic Regression\n",
        "\n",
        "We\u2019ll now derive the gradient for a **softmax classifier** trained with **cross-entropy loss**.\n",
        "\n",
        "### Model Output\n",
        "\n",
        "The model computes:\n",
        "\n",
        "```\n",
        "z = w \u00b7 x + b         # raw scores (logits)\n",
        "\u0177 = softmax(z)        # predicted probabilities\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `x` is the input vector (length `n`)\n",
        "- `w` is a weight matrix with shape `k \u00d7 n` (k = number of classes)\n",
        "- `b` is a bias vector (length `k`)\n",
        "- `\u0177` is a vector of class probabilities (length `k`)\n",
        "\n",
        "The softmax function for class `i` is:\n",
        "\n",
        "```\n",
        "\u0177\u1d62 = exp(z\u1d62) / sum_j exp(z\u2c7c)\n",
        "```\n",
        "\n",
        "### Loss Function: Cross-Entropy\n",
        "\n",
        "If the true class label is `y` (an integer from `0` to `k-1`), the loss is:\n",
        "\n",
        "```\n",
        "L = -log(\u0177_y)        # negative log of the predicted probability for the true class\n",
        "```\n",
        "\n",
        "### Goal\n",
        "\n",
        "We want to compute the gradients:\n",
        "\n",
        "- \u2202L/\u2202w\u1d62\u2c7c \u2014 how the loss changes with respect to each weight\n",
        "- \u2202L/\u2202b\u1d62  \u2014 how the loss changes with respect to each bias\n",
        "\n",
        "### Gradient of the Loss\n",
        "\n",
        "We apply the chain rule. The derivative of the loss with respect to the logit `z\u1d62` is:\n",
        "\n",
        "```\n",
        "\u2202L/\u2202z\u1d62 = \u0177\u1d62 - 1   if i == y\n",
        "\u2202L/\u2202z\u1d62 = \u0177\u1d62       otherwise\n",
        "```\n",
        "\n",
        "This tells us that:\n",
        "\n",
        "- For the **correct class**, we subtract 1 from the predicted probability\n",
        "- For all other classes, the gradient is just the predicted probability\n",
        "\n",
        "### Final Gradients\n",
        "\n",
        "Now that we have \u2202L/\u2202z\u1d62, we compute the gradients with respect to the weights and biases:\n",
        "\n",
        "```\n",
        "\u2202L/\u2202w\u1d62\u2c7c = (\u0177\u1d62 - 1[y = i]) * x\u2c7c\n",
        "\u2202L/\u2202b\u1d62  = (\u0177\u1d62 - 1[y = i])\n",
        "```\n",
        "\n",
        "Where `1[y = i]` is 1 if `i` is the correct class, otherwise 0.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- The **error** for each class is:  \n",
        "  ```\n",
        "  error\u1d62 = \u0177\u1d62 - 1[y = i]\n",
        "  ```\n",
        "- We multiply that error by each input feature to get the gradient:\n",
        "  ```\n",
        "  dw\u1d62\u2c7c = error\u1d62 * x\u2c7c\n",
        "  db\u1d62  = error\u1d62\n",
        "  ```\n",
        "\n",
        "This gives us the gradients we need to perform gradient descent on a multiclass softmax classifier.\n",
        "\n",
        "## SMS Spam Collection\n",
        "\n",
        "To see how logistic regressions work, let's try building one. We'll use the same dataset we used in Week 4. \n",
        "\n",
        "### Dataset, Tokenizer, and Vectorizer\n",
        "\n",
        "First, we'll download the data and split it into test and train datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget https://wd13ca.github.io/BAN200-Summer-2025/SMSSpamCollection.txt\n",
        "\n",
        "# Load it into a pandas DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"SMSSpamCollection.txt\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset: 80% training, 20% testing\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Show the number of messages in each set\n",
        "print(\"Training messages:\", len(train_df))\n",
        "print(\"Test messages:\", len(test_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Next, we'll recreate our tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "def tokenize(text):\n",
        "    lowercase_text = text.lower()\n",
        "    tokens = re.findall(r'\\b\\w+\\b', lowercase_text)\n",
        "    return [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We'll also need a vectorizer. Let's use the one from Week 3. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import log\n",
        "\n",
        "N = len(train_df) # total number of documents\n",
        "\n",
        "doc_freq = {} # document frequency for each word\n",
        "for _, row in train_df.iterrows():\n",
        "  tokens = tokenize(row[\"message\"])\n",
        "  unique_tokens = set(tokens) # only count once per document\n",
        "  for token in unique_tokens:\n",
        "      doc_freq[token] = doc_freq.get(token, 0) + 1\n",
        "\n",
        "idf_dict = {}\n",
        "for token, df in doc_freq.items():\n",
        "    idf_dict[token] = log(N / df)\n",
        "\n",
        "def vectorize_tf_idf(tokens):\n",
        "    \"\"\"\n",
        "    Takes a list of tokens and an IDF dictionary,\n",
        "    returns a dictionary representing the TF-IDF vector.\n",
        "    \"\"\"\n",
        "    tf = {}\n",
        "    for token in tokens:\n",
        "        if token in tf:\n",
        "            tf[token] += 1\n",
        "        else:\n",
        "            tf[token] = 1\n",
        "\n",
        "    tf_idf = {}\n",
        "    for token, freq in tf.items():\n",
        "        if token in idf_dict:\n",
        "            tf_idf[token] = freq * idf_dict[token]\n",
        "\n",
        "    return tf_idf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Defining a Model Class\n",
        "\n",
        "To build a multiclass classification model from scratch, we\u2019ll use **multiclass logistic regression** with:\n",
        "\n",
        "- A **softmax activation function** to produce probabilities\n",
        "- A **cross-entropy loss** to measure how wrong the predictions are\n",
        "- **Gradient descent** to update the weights and bias\n",
        "\n",
        "We\u2019ll define a Python class called `BoWLogisticClassifier` to encapsulate everything: prediction (`forward`), gradient computation (`backward`), and model parameters (`w` and `b`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BoWLogisticClassifier:\n",
        "    \"\"\"\n",
        "    A sparse multiclass logistic regression model with softmax activation.\n",
        "    Input vectors x are sparse dictionaries: {feature: value}.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab, classes):\n",
        "        \"\"\"\n",
        "        Initialize model with zero weights and zero biases.\n",
        "        - vocab: a set of input feature names (e.g., words)\n",
        "        - classes: a set of class labels (e.g., 'spam', 'ham')\n",
        "        \"\"\"\n",
        "        self.vocab = sorted(vocab)\n",
        "        self.classes = sorted(classes)\n",
        "        self.input_dim = len(self.vocab)\n",
        "        self.output_dim = len(self.classes)\n",
        "\n",
        "        # Map vocab and classes to index\n",
        "        self.vocab_index = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.class_index = {label: i for i, label in enumerate(self.classes)}\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.w = [[0.0 for _ in range(self.input_dim)] for _ in range(self.output_dim)]\n",
        "        self.b = [0.0 for _ in range(self.output_dim)]\n",
        "\n",
        "    def softmax(self, logits):\n",
        "        \"\"\"\n",
        "        Apply softmax to logits for numerical stability.\n",
        "        \"\"\"\n",
        "        max_logit = max(logits)\n",
        "        exp_scores = [math.exp(z - max_logit) for z in logits]\n",
        "        sum_exp = sum(exp_scores)\n",
        "        return [s / sum_exp for s in exp_scores]\n",
        "\n",
        "    def forward(self, x_sparse):\n",
        "        \"\"\"\n",
        "        Compute prediction for sparse input x_sparse: dict {feature: value}.\n",
        "        Returns probability distribution over classes.\n",
        "        \"\"\"\n",
        "        logits = []\n",
        "        for k in range(self.output_dim):\n",
        "            score = self.b[k]\n",
        "            for word, value in x_sparse.items():\n",
        "                if word in self.vocab_index:\n",
        "                    j = self.vocab_index[word]\n",
        "                    score += self.w[k][j] * value\n",
        "            logits.append(score)\n",
        "        return self.softmax(logits)\n",
        "\n",
        "    def backward(self, x_sparse, y_true_label):\n",
        "        \"\"\"\n",
        "        Compute gradients w.r.t weights and biases using cross-entropy loss.\n",
        "        - x_sparse: input features as dict {feature: value}\n",
        "        - y_true_label: true class label (e.g., 'spam')\n",
        "\n",
        "        Returns:\n",
        "            dw: sparse gradient of weights (dict of dicts)\n",
        "            db: dense gradient of biases (list)\n",
        "        \"\"\"\n",
        "        y_true = self.class_index[y_true_label]\n",
        "        y_pred = self.forward(x_sparse)\n",
        "\n",
        "        # Gradients\n",
        "        dw = {k: {} for k in range(self.output_dim)}\n",
        "        db = [0.0 for _ in range(self.output_dim)]\n",
        "\n",
        "        for k in range(self.output_dim):\n",
        "            error = y_pred[k] - (1 if k == y_true else 0)\n",
        "            for word, value in x_sparse.items():\n",
        "                if word in self.vocab_index:\n",
        "                    j = self.vocab_index[word]\n",
        "                    dw[k][word] = error * value\n",
        "            db[k] = error\n",
        "\n",
        "        return dw, db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Training the Classifier\n",
        "\n",
        "Now let\u2019s train our logistic regression model using gradient descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Build vocabulary and class labels from training set\n",
        "all_tokens = [token for message in train_df[\"message\"] for token in tokenize(message)]\n",
        "vocab = set(all_tokens)\n",
        "classes = set(train_df[\"label\"])\n",
        "\n",
        "# Initialize model\n",
        "model = BoWLogisticClassifier(vocab=vocab, classes=classes)\n",
        "\n",
        "# Prepare training data as (x_sparse, y_label) pairs\n",
        "train_data = []\n",
        "for _, row in train_df.iterrows():\n",
        "    tokens = tokenize(row[\"message\"])\n",
        "    x_sparse = vectorize_tf_idf(tokens)\n",
        "    y_label = row[\"label\"]\n",
        "    train_data.append((x_sparse, y_label))\n",
        "\n",
        "# Training loop\n",
        "def train(model, data, lr=0.1, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        random.shuffle(data)\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x_sparse, y_label in data:\n",
        "            y_true_idx = model.class_index[y_label]\n",
        "            y_pred = model.forward(x_sparse)\n",
        "\n",
        "            # Cross-entropy loss\n",
        "            loss = -math.log(y_pred[y_true_idx] + 1e-12)\n",
        "            total_loss += loss\n",
        "\n",
        "            # Compute gradients\n",
        "            dw, db = model.backward(x_sparse, y_label)\n",
        "\n",
        "            # Update weights and biases\n",
        "            for k in range(model.output_dim):\n",
        "                for word, grad in dw[k].items():\n",
        "                    j = model.vocab_index[word]\n",
        "                    model.w[k][j] -= lr * grad\n",
        "                model.b[k] -= lr * db[k]\n",
        "\n",
        "        avg_loss = total_loss / len(data)\n",
        "        print(f\"Epoch {epoch + 1}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, train_data, lr=0.1, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Predict Function\n",
        "\n",
        "Let's create a `predict()` function that takes an unlabeled message and returns a predicted label:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(message, model, tokenizer, vectorizer):\n",
        "    \"\"\"\n",
        "    Predict the class label for a raw input message (string).\n",
        "    - message: the input message (e.g., \"Free entry now!!!\")\n",
        "    - tokenizer: a function that tokenizes the message\n",
        "    - vectorizer: a function that turns tokens into sparse vector\n",
        "\n",
        "    Returns:\n",
        "        predicted_label: the class label with highest probability\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(message)\n",
        "    x_sparse = vectorizer(tokens)\n",
        "    probs = model.forward(x_sparse)\n",
        "    predicted_idx = probs.index(max(probs))\n",
        "    return model.classes[predicted_idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Evaluate Model Accuracy on the Test Set\n",
        "\n",
        "Now that we have a working `predict()` function, we\u2019ll apply it to every message in the test set.\n",
        "\n",
        "Then we\u2019ll compare the predicted labels to the actual labels and calculate the model\u2019s accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict all messages in the test set\n",
        "predictions = []\n",
        "\n",
        "for _, row in test_df.iterrows():\n",
        "    message = row[\"message\"]\n",
        "    prediction = predict(message,model,tokenize,vectorize_tf_idf)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Actual labels\n",
        "actual = test_df[\"label\"].tolist()\n",
        "\n",
        "# Compute accuracy\n",
        "correct = sum([pred == truth for pred, truth in zip(predictions, actual)])\n",
        "accuracy = correct / len(test_df)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Confusion Matrix\n",
        "\n",
        "Now let's take a look at the confusion matrix:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(actual, predictions, labels=[\"spam\", \"ham\"])\n",
        "\n",
        "# Display as a readable table\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"               Predicted\")\n",
        "print(f\"             | spam | ham \")\n",
        "print(f\"Actual spam  |  {cm[0][0]:4} | {cm[0][1]:4}\")\n",
        "print(f\"Actual ham   |  {cm[1][0]:4} | {cm[1][1]:4}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Precision, Recall, and F1-Score\n",
        "\n",
        "Here are the precision, recall, and F1-scores:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(actual, predictions, target_names=[\"spam\", \"ham\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Error Analysis\n",
        "\n",
        "Let\u2019s look at some misclassified messages \u2014 where the model's prediction didn't match the true label.\n",
        "\n",
        "This helps us understand:\n",
        "\n",
        "- Where the model is confused\n",
        "- Whether certain types of spam are being missed\n",
        "- If it\u2019s too aggressive (labeling ham as spam)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the first 10 misclassified messages\n",
        "for i in range(len(test_df)):\n",
        "    if predictions[i] != actual[i]:\n",
        "        print(f\"\\n--- Misclassified Message ---\")\n",
        "        print(f\"Actual:    {actual[i]}\")\n",
        "        print(f\"Predicted: {predictions[i]}\")\n",
        "        print(f\"Message:   {test_df.iloc[i]['message']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression vs. Naive Bayes: Why Logistic Regression Sometimes Performs Worse\n",
        "\n",
        "You may have noticed that our **logistic regression model** doesn't perform quite as well as the **Naive Bayes model** on the SMS Spam Collection dataset. This might seem surprising at first \u2014 after all, logistic regression is more flexible and learned its weights directly from the data. So what\u2019s going on?\n",
        "\n",
        "### Why Logistic Regression Might Underperform\n",
        "\n",
        "Here are a few possible reasons:\n",
        "\n",
        "1. **Naive Bayes is surprisingly strong for text classification**  \n",
        "   Even though it makes strong independence assumptions (i.e. that words are independent given the class), those assumptions actually hold *reasonably well* in many text datasets. This makes Naive Bayes hard to beat, especially on short documents like SMS messages.\n",
        "\n",
        "2. **Logistic regression is more sensitive to sparse features**  \n",
        "   Logistic regression must learn one weight per feature, and when the vocabulary is large and many features appear rarely (as in text), it can be harder for the model to learn stable estimates \u2014 especially with limited training data.\n",
        "\n",
        "3. **No regularization in our implementation**  \n",
        "   Our logistic regression model is very basic: it doesn\u2019t use techniques like **L2 regularization**, which can prevent overfitting and improve generalization \u2014 especially with high-dimensional sparse inputs like TF-IDF vectors.\n",
        "\n",
        "4. **Naive Bayes builds in strong priors**  \n",
        "   Naive Bayes directly incorporates class priors (how common spam vs. ham messages are), and it\u2019s especially effective when those priors are unbalanced. Logistic regression can learn this too, but it requires more data and care.\n",
        "\n",
        "### Why Learn Logistic Regression Anyway?\n",
        "\n",
        "Even though Naive Bayes outperforms logistic regression *here*, logistic regression is still **worth learning \u2014 and essential going forward**:\n",
        "\n",
        "- It introduces the idea of **learning weights from data**, not from counts or rules  \n",
        "- It teaches the concept of **gradient descent**, which we\u2019ll use to train more complex models  \n",
        "- It prepares us for **deep learning**, where logistic regression is effectively the **final layer** of most neural networks used for classification  \n",
        "- It scales better to complex, high-dimensional data once regularization and optimization improvements are added\n",
        "\n",
        "> **Key takeaway:**  \n",
        "> Logistic regression might not win on this dataset \u2014 but it\u2019s a critical stepping stone toward more powerful and general models, like deep neural networks and transformers.\n",
        "\n",
        "## Group Project Reminder\n",
        "\n",
        "Project Proposals are due next week. Class time will be used for groups to meet with the Professor to discuss their Project Proposals. \n",
        "\n",
        "## Midterm Reminder\n",
        "\n",
        "The Midterm will take place the week after Study Week. There will be no class that week. The Midterm will be online and open book. Students will have the entire week to complete it. It will cover all lecture material. (Material covered in the textbook will not be covered directly on the midterm - the textbook is just a resource to help you understand the code used in class.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}